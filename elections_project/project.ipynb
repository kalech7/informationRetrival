{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entrevistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo convertido a CSV con ,: ./data/interview/Leonidas Iza.csv\n",
      "Archivo convertido a CSV con ,: ./data/interview/HenryKronfle.csv\n",
      "Archivo convertido a CSV con ,: ./data/interview/Luis Tilleria.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Archivos Excel de entrada\n",
    "archivos_excel = [\n",
    "    './data/interview/Leonidas Iza.xlsx',\n",
    "    './data/interview/HenryKronfle.xlsx',\n",
    "    './data/interview/Luis Tilleria.xlsx'\n",
    "]\n",
    "\n",
    "# Convertir cada archivo Excel a CSV con ,\n",
    "for archivo_excel in archivos_excel:\n",
    "    # Leer el archivo Excel\n",
    "    df = pd.read_excel(archivo_excel)\n",
    "    \n",
    "    # Definir el archivo CSV de salida\n",
    "    archivo_csv = archivo_excel.replace('.xlsx', '.csv')  # Reemplaza la extensi√≥n .xlsx por .csv\n",
    "    \n",
    "    # Guardar como archivo CSV con ,\n",
    "    df.to_csv(archivo_csv, index=False, sep=',')  # Usamos tabulaci√≥n como separador\n",
    "    \n",
    "    print(f\"Archivo convertido a CSV con ,: {archivo_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo procesado: ./data/interview/Leonidas_Iza_pross.csv\n",
      "‚úÖ Archivo procesado: ./data/interview/Henry_Kronfle_pross.csv\n",
      "‚úÖ Archivo procesado: ./data/interview/Luis_Tilleria_pross.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import ollama  # Aseg√∫rate de tener instalada la librer√≠a con: pip install ollama\n",
    "\n",
    "# Lista de temas relevantes\n",
    "temas_relevantes = { \n",
    "    \"econom√≠a\", \"educaci√≥n\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupci√≥n\", \"tecnolog√≠a\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"pol√≠tica\", \"desarrollo\", \"energ√≠a\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovaci√≥n\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversi√≥n\",\n",
    "    \"vivienda\", \"servicios p√∫blicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educaci√≥n superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalizaci√≥n\", \"gesti√≥n p√∫blica\", \"comercio\",\n",
    "    \"cambio clim√°tico\", \"energ√≠as renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud p√∫blica\", \"gobernanza\", \"justicia social\", \"igualdad de g√©nero\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestaci√≥n\", \"movilidad urbana\", \"biodiversidad\", \"educaci√≥n financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnol√≥gica\",\n",
    "    \"educaci√≥n digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnolog√≠a\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"econom√≠a circular\",\n",
    "    \"ciudades inteligentes\", \"protecci√≥n de datos\", \"energ√≠a solar\", \"transporte el√©ctrico\",\n",
    "    \"robotizaci√≥n\", \"computaci√≥n cu√°ntica\", \"espacio exterior\", \"protecci√≥n ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad el√©ctrica\", \"alimentos org√°nicos\", \"tecnolog√≠a educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas aut√≥nomos\",\n",
    "    \"tecnolog√≠a espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energ√≠a e√≥lica\", \"tecnolog√≠as disruptivas\", \"energ√≠a geot√©rmica\",\n",
    "    \"nanotecnolog√≠a\", \"microbioma\", \"bioeconom√≠a\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energ√≠a limpia\", \"criptomonedas\", \"miner√≠a digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educaci√≥n en l√≠nea\",\n",
    "    \"bio\", \"ecoinnovaci√≥n\", \"simulaci√≥n computacional\", \"agricultura urbana\", \"cultivos inteligentes\",\"IESS\"\n",
    "}\n",
    "\n",
    "# üìå Funci√≥n para extraer los temas tratados\n",
    "def extraer_temas(texto):\n",
    "    # Aseguramos que el texto no sea vac√≠o o nulo\n",
    "    if not texto or not isinstance(texto, str):\n",
    "        return \"Otros\"\n",
    "    \n",
    "    # Convertimos el texto a min√∫sculas y buscamos los temas\n",
    "    texto = texto.lower()\n",
    "    temas_detectados = [tema for tema in temas_relevantes if tema.lower() in texto]\n",
    "    \n",
    "    # Si encontramos temas, los unimos en una cadena separada por comas\n",
    "    return \", \".join(temas_detectados) if temas_detectados else \"Otros\"\n",
    "\n",
    "# üìå Funci√≥n para generar res√∫menes con Ollama\n",
    "def generar_resumen_ollama(texto):\n",
    "    prompt = f\"Resume el siguiente texto en un parrafo:\\n\\n{texto}\"\n",
    "\n",
    "    try:\n",
    "        respuesta = ollama.chat(\n",
    "            model=\"llama3.2:latest\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "        )\n",
    "        resumen = respuesta.get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "        # Eliminar saltos de l√≠nea y espacios extras\n",
    "        resumen = ' '.join(resumen.split())\n",
    "        return resumen if resumen else \"No se pudo generar el resumen\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error con Ollama: {e}\")\n",
    "        return \"Resumen no disponible\"\n",
    "\n",
    "def procesar_csv(archivo_entrada, archivo_salida):\n",
    "    df = pd.read_csv(archivo_entrada,sep=\",\")\n",
    "\n",
    "    # Verifica si la columna \"entrevista\" existe en el archivo CSV\n",
    "    if \"entrevista\" not in df.columns:\n",
    "        print(\"‚ùå La columna 'entrevista' no se encuentra en el archivo.\")\n",
    "        return\n",
    "\n",
    "    # Extraer temas tratados\n",
    "    df[\"Temas_Tratados\"] = df[\"entrevista\"].astype(str).apply(extraer_temas)\n",
    "\n",
    "    # Generar res√∫menes con Ollama\n",
    "    df[\"descripcion\"] = df[\"entrevista\"].astype(str).apply(generar_resumen_ollama)\n",
    "\n",
    "    # Guardar el archivo procesado\n",
    "    df.to_csv(archivo_salida, index=False, encoding=\"utf-8-sig\", sep=\",\")\n",
    "    print(f\"‚úÖ Archivo procesado: {archivo_salida}\")\n",
    "\n",
    "# üî• Ejecutar para m√∫ltiples archivos CSV\n",
    "procesar_csv(\"./data/interview/Leonidas Iza.csv\", \"./data/interview/Leonidas_Iza_pross.csv\")\n",
    "procesar_csv(\"./data/interview/HenryKronfle.csv\", \"./data/interview/Henry_Kronfle_pross.csv\")\n",
    "procesar_csv(\"./data/interview/Luis Tilleria.csv\", \"./data/interview/Luis_Tilleria_pross.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Andrea_Gonzalez.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": carlos_rabascall.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Daniel_Noboa.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": HenryKronfle.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Henry_Kronfle_pross.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": JimmyJairala.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": JorgeEscala.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Leonidas Iza.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Leonidas_Iza_pross.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Luis Tilleria.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Luis_Tilleria_pross.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": pedro_granja.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": victor_araus.csv\n",
      "Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": Wilson_Gomez.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:26: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\55085014.py:24: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# Carpeta que contiene los archivos CSV\n",
    "directorio_csv = './data/interview'  # Cambia por la ruta de tu carpeta con archivos CSV\n",
    "\n",
    "# Definir las columnas en el orden que quieres\n",
    "columnas_ordenadas = ['ID', 'Candidato', 'Temas_Tratados', 'Descripcion', 'Entrevista']\n",
    "\n",
    "# Funci√≥n para encontrar el nombre m√°s similar a una lista de nombres\n",
    "def encontrar_columna_similar(nombre_columna, lista_columnas):\n",
    "    import difflib\n",
    "    # Encuentra la coincidencia m√°s cercana\n",
    "    coincidencias = difflib.get_close_matches(nombre_columna, lista_columnas, n=1, cutoff=0.8)\n",
    "    return coincidencias[0] if coincidencias else None\n",
    "\n",
    "# Funci√≥n para depurar un DataFrame\n",
    "def depurar_dataframe(df):\n",
    "    # Rellenar valores NaN con un valor vac√≠o o 0 dependiendo del tipo de la columna\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Si es columna de tipo texto\n",
    "            df[col].fillna('', inplace=True)\n",
    "        else:  # Si es columna num√©rica\n",
    "            df[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Verificar si todas las columnas esperadas est√°n presentes\n",
    "    for col in columnas_ordenadas:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''  # Si falta alguna columna, agregarla como vac√≠a\n",
    "    \n",
    "    # Asegurarse de que el DataFrame no est√© vac√≠o\n",
    "    if df.empty:\n",
    "        print(\"El DataFrame est√° vac√≠o, no se guardar√°.\")\n",
    "        return None\n",
    "    \n",
    "    # Asignar numeraci√≥n secuencial a la columna 'ID'\n",
    "    df['ID'] = range(1, len(df) + 1)\n",
    "    \n",
    "    # Reordenar las columnas del DataFrame\n",
    "    df = df[columnas_ordenadas]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta\n",
    "for archivo in os.listdir(directorio_csv):\n",
    "    if archivo.endswith('.csv'):\n",
    "        archivo_csv = os.path.join(directorio_csv, archivo)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(archivo_csv, sep=',', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo {archivo}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Renombrar las columnas si hay coincidencias\n",
    "        columnas_actuales = df.columns.tolist()\n",
    "        columnas_renombradas = {}\n",
    "        \n",
    "        for col in columnas_actuales:\n",
    "            columna_similar = encontrar_columna_similar(col, columnas_ordenadas)\n",
    "            if columna_similar:\n",
    "                columnas_renombradas[col] = columna_similar\n",
    "        \n",
    "        # Renombrar las columnas en el DataFrame\n",
    "        df.rename(columns=columnas_renombradas, inplace=True)\n",
    "        \n",
    "        # Depurar el DataFrame\n",
    "        df_limpio = depurar_dataframe(df)\n",
    "        \n",
    "        if df_limpio is not None:\n",
    "            # Guardar el archivo CSV con el nuevo orden de columnas y delimitador \";\"\n",
    "            df_limpio.to_csv(archivo_csv, index=False, sep=',')\n",
    "            print(f'Archivo renombrado, ordenado y con numeraci√≥n secuencial en \"ID\": {archivo}')\n",
    "        else:\n",
    "            print(f'El archivo {archivo} no se guard√≥ debido a que est√° vac√≠o o con errores.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unir entrevistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado como: ./data/interview/archivo_combinado.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:25: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(0, inplace=True)\n",
      "C:\\Users\\kale\\AppData\\Local\\Temp\\ipykernel_6128\\3395417661.py:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna('', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "# Carpeta que contiene los archivos CSV\n",
    "directorio_csv = './data/interview/'  # Cambia por la ruta de tu carpeta con archivos CSV\n",
    "\n",
    "# Definir las columnas en el orden que quieres\n",
    "columnas_ordenadas = ['ID', 'Candidato', 'Temas_Tratados', 'Descripcion', 'Entrevista']\n",
    "\n",
    "# Funci√≥n para encontrar el nombre m√°s similar a una lista de nombres\n",
    "def encontrar_columna_similar(nombre_columna, lista_columnas):\n",
    "    import difflib\n",
    "    # Encuentra la coincidencia m√°s cercana\n",
    "    coincidencias = difflib.get_close_matches(nombre_columna, lista_columnas, n=1, cutoff=0.8)\n",
    "    return coincidencias[0] if coincidencias else None\n",
    "\n",
    "# Funci√≥n para depurar un DataFrame\n",
    "def depurar_dataframe(df):\n",
    "    # Rellenar valores NaN con un valor vac√≠o o 0 dependiendo del tipo de la columna\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object':  # Si es columna de tipo texto\n",
    "            df[col].fillna('', inplace=True)\n",
    "        else:  # Si es columna num√©rica\n",
    "            df[col].fillna(0, inplace=True)\n",
    "    \n",
    "    # Verificar si todas las columnas esperadas est√°n presentes\n",
    "    for col in columnas_ordenadas:\n",
    "        if col not in df.columns:\n",
    "            df[col] = ''  # Si falta alguna columna, agregarla como vac√≠a\n",
    "    \n",
    "    # Asegurarse de que el DataFrame no est√© vac√≠o\n",
    "    if df.empty:\n",
    "        print(\"El DataFrame est√° vac√≠o, no se guardar√°.\")\n",
    "        return None\n",
    "    \n",
    "    # Reordenar las columnas del DataFrame\n",
    "    df = df[columnas_ordenadas]\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Lista para almacenar todos los DataFrames\n",
    "df_final = []\n",
    "id_contador = 1  # Contador para los IDs secuenciales\n",
    "\n",
    "# Iterar sobre todos los archivos en la carpeta\n",
    "for archivo in os.listdir(directorio_csv):\n",
    "    if archivo.endswith('.csv'):\n",
    "        archivo_csv = os.path.join(directorio_csv, archivo)\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(archivo_csv, sep=',', quoting=csv.QUOTE_MINIMAL, quotechar='\"')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al leer el archivo {archivo}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # Renombrar las columnas si hay coincidencias\n",
    "        columnas_actuales = df.columns.tolist()\n",
    "        columnas_renombradas = {}\n",
    "        \n",
    "        for col in columnas_actuales:\n",
    "            columna_similar = encontrar_columna_similar(col, columnas_ordenadas)\n",
    "            if columna_similar:\n",
    "                columnas_renombradas[col] = columna_similar\n",
    "        \n",
    "        # Renombrar las columnas en el DataFrame\n",
    "        df.rename(columns=columnas_renombradas, inplace=True)\n",
    "        \n",
    "        # Depurar el DataFrame\n",
    "        df_limpio = depurar_dataframe(df)\n",
    "        \n",
    "        if df_limpio is not None:\n",
    "            # Asignar los IDs secuenciales antes de agregar el DataFrame al archivo final\n",
    "            df_limpio['ID'] = range(id_contador, id_contador + len(df_limpio))\n",
    "            id_contador += len(df_limpio)  # Incrementar el contador para el siguiente archivo\n",
    "            \n",
    "            # Agregar el DataFrame limpio a la lista\n",
    "            df_final.append(df_limpio)\n",
    "        else:\n",
    "            print(f'El archivo {archivo} no se guard√≥ debido a que est√° vac√≠o o con errores.')\n",
    "\n",
    "# Combinar todos los DataFrames en uno solo\n",
    "df_combinado = pd.concat(df_final, ignore_index=True)\n",
    "\n",
    "# Guardar el archivo combinado en un nuevo CSV\n",
    "archivo_combinado = './data/interview/archivo_combinado.csv'\n",
    "df_combinado.to_csv(archivo_combinado, index=False, sep=',')\n",
    "\n",
    "print(f'Archivo combinado guardado como: {archivo_combinado}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo expandido guardado como: ./data/interview/archivo_combinado_expandido.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Funci√≥n para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    if not isinstance(text, str):\n",
    "        return ''  # Si no es un string, devolver vac√≠o\n",
    "    \n",
    "    # Eliminar vi√±etas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (n√∫meros seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la l√≠nea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Funci√≥n para dividir el texto en oraciones por ID\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    \n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    \n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con ID y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Cargar el archivo combinado\n",
    "archivo_combinado = './data/interview/archivo_combinado.csv'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(archivo_combinado)\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Error: No se encontr√≥ el archivo {archivo_combinado}\")\n",
    "    exit()\n",
    "except pd.errors.EmptyDataError:\n",
    "    print(f\"‚ùå Error: El archivo {archivo_combinado} est√° vac√≠o.\")\n",
    "    exit()\n",
    "\n",
    "# Verificar si la columna 'ID' ya existe\n",
    "if 'ID' in df.columns:\n",
    "    df = df.sort_values(by='ID').reset_index(drop=True)  # Ordenar por ID y resetear √≠ndices\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Advertencia: La columna 'ID' no existe. Se generar√° numeraci√≥n autom√°tica.\")\n",
    "    df['ID'] = range(1, len(df) + 1)\n",
    "\n",
    "# Crear lista para almacenar las nuevas filas con oraciones separadas\n",
    "nuevas_filas = []\n",
    "\n",
    "# Procesar cada fila del dataframe\n",
    "for _, row in df.iterrows():\n",
    "    text_id = row['ID']\n",
    "    texto_limpio = clean_content(row.get('Entrevista', ''))  # Limpiar el texto antes de dividirlo\n",
    "    oraciones = dividir_oraciones_por_id(texto_limpio, text_id)  # Extraer oraciones con ID\n",
    "    \n",
    "    for id_original, id_oracion, oracion in oraciones:\n",
    "        nueva_fila = row.drop(labels=['Entrevista'], errors='ignore').to_dict()  # Evitar KeyError\n",
    "        nueva_fila['ID_Oracion'] = f\"{id_original}-{id_oracion}\"  # ID √∫nico para cada oraci√≥n\n",
    "        nueva_fila['Oracion_Entrevista'] = oracion\n",
    "        nuevas_filas.append(nueva_fila)\n",
    "\n",
    "# Convertir la lista de nuevas filas en un DataFrame\n",
    "df_expandido = pd.DataFrame(nuevas_filas)\n",
    "\n",
    "# Reajustar la columna ID para que sea secuencial\n",
    "df_expandido['ID'] = range(1, len(df_expandido) + 1)\n",
    "\n",
    "# Guardar el archivo actualizado con oraciones separadas\n",
    "archivo_expandido = './data/interview/archivo_combinado_expandido.csv'\n",
    "df_expandido.to_csv(archivo_expandido, index=False, sep=',')\n",
    "\n",
    "print(f'‚úÖ Archivo expandido guardado como: {archivo_expandido}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeros valores de 'Partido' en oracion_df:\n",
      "0    REVOLUCI√ìN CIUDADANA - RETO\n",
      "1    REVOLUCI√ìN CIUDADANA - RETO\n",
      "2    REVOLUCI√ìN CIUDADANA - RETO\n",
      "3    REVOLUCI√ìN CIUDADANA - RETO\n",
      "4    REVOLUCI√ìN CIUDADANA - RETO\n",
      "Name: Partido, dtype: object\n",
      "Primeros valores de 'Partido' en completo_df:\n",
      "0    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "1    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "2    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "3    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "4    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "Name: Partido, dtype: object\n",
      "Primeras filas del archivo fusionado:\n",
      "    id  oracion_id                                   oracion_original  \\\n",
      "0  AG1           1  Andrea Gonz√°lez Nader, candidata presidencial ...   \n",
      "1  AG1           2  Propone una reducci√≥n del aparato estatal medi...   \n",
      "2  AG1           3  En materia de seguridad, destaca la necesidad ...   \n",
      "3  AG1           4  Adem√°s, promueve una reforma constitucional si...   \n",
      "4  AG1           5  En el √°mbito de salud, propone un enfoque prev...   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "0  andrea gonzalez nader candidata presidencial p...   \n",
      "1  propone una reduccion del aparato estatal medi...   \n",
      "2  en materia de seguridad destaca la necesidad d...   \n",
      "3  ademas promueve una reforma constitucional sin...   \n",
      "4  en el ambito de salud propone un enfoque preve...   \n",
      "\n",
      "                                oracion_sinStopWords       presidente  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...  ANDREA GONZALEZ   \n",
      "1  propone reduccion aparato estatal mediante eli...  ANDREA GONZALEZ   \n",
      "2  materia seguridad destaca necesidad retomar co...  ANDREA GONZALEZ   \n",
      "3  ademas promueve reforma constitucional recurri...  ANDREA GONZALEZ   \n",
      "4  ambito salud propone enfoque preventivo lugar ...  ANDREA GONZALEZ   \n",
      "\n",
      "  vicepresidente  lista                                  Partido Oracion  \\\n",
      "0   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "1   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "2   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "3   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "4   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO     NaN   \n",
      "\n",
      "  Temas Clave  \n",
      "0         NaN  \n",
      "1         NaN  \n",
      "2         NaN  \n",
      "3         NaN  \n",
      "4         NaN  \n",
      "Proceso completado. Archivo guardado como 'partidopol.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los archivos\n",
    "oracion_df = pd.read_csv('oraciones_procesadas_completo.csv', sep=\";\")\n",
    "completo_df = pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "\n",
    "# Verificar los primeros valores de la columna 'Partido' para ambos DataFrames\n",
    "print(\"Primeros valores de 'Partido' en oracion_df:\")\n",
    "print(oracion_df['Partido'].head())\n",
    "print(\"Primeros valores de 'Partido' en completo_df:\")\n",
    "print(completo_df['Partido'].head())\n",
    "\n",
    "# Asegurarse de que las columnas 'Partido' sean del mismo tipo (string)\n",
    "oracion_df['Partido'] = oracion_df['Partido'].astype(str)\n",
    "completo_df['Partido'] = completo_df['Partido'].astype(str)\n",
    "\n",
    "# Excluir las columnas no deseadas de 'Oracion.csv'\n",
    "columnas_excluir = ['ID', 'Oracion_ID', 'CandidatoPresidente', 'CandidatoVicePresidente', 'ListaPolitica']\n",
    "oracion_df_limpio = oracion_df.drop(columns=columnas_excluir)\n",
    "\n",
    "# Fusionar por la columna 'Partido' y agregar las columnas 'Oracion' y 'Temas Clave'\n",
    "completo_df = completo_df.merge(oracion_df_limpio[['Partido', 'Oracion', 'Temas Clave']], on='Partido', how='left')\n",
    "\n",
    "# Verificar la fusi√≥n\n",
    "print(\"Primeras filas del archivo fusionado:\")\n",
    "print(completo_df.head())\n",
    "\n",
    "# Guardar el archivo actualizado\n",
    "completo_df.to_csv(\"partidopol.csv\", index=False)\n",
    "\n",
    "print(\"Proceso completado. Archivo guardado como 'partidopol.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    REVOLUCI√ìN CIUDADANA - RETO\n",
      "1    REVOLUCI√ìN CIUDADANA - RETO\n",
      "2    REVOLUCI√ìN CIUDADANA - RETO\n",
      "3    REVOLUCI√ìN CIUDADANA - RETO\n",
      "4    REVOLUCI√ìN CIUDADANA - RETO\n",
      "Name: Partido, dtype: object\n",
      "0    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "1    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "2    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "3    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "4    PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO\n",
      "Name: Partido, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# Cargar los archivos\n",
    "oracion_df = pd.read_csv('oraciones_procesadas_completo.csv', sep=\";\")\n",
    "completo_df = pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "print(oracion_df['Partido'].head())\n",
    "print(completo_df['Partido'].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde est√°n los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios espec√≠ficos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCI√ìN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA M√ÅS ACCI√ìN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCR√ÅTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCR√ÅTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRI√ìTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Funci√≥n para obtener el √∫ltimo ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios espec√≠ficos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde est√°n los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios espec√≠ficos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCI√ìN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA M√ÅS ACCI√ìN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCR√ÅTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCR√ÅTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRI√ìTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Funci√≥n para obtener el √∫ltimo ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Funci√≥n para extraer texto del PDF excluyendo las primeras y √∫ltimas p√°ginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Funci√≥n para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar vi√±etas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (n√∫meros seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la l√≠nea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Funci√≥n para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios espec√≠ficos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extra√≠do\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuraci√≥n global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a im√°genes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID √∫nico a cada oraci√≥n\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID √∫nico para cada oraci√≥n\n",
    "            oraciones_texto.append(oracion)  # Texto de la oraci√≥n\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA S√ç _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA S√ç\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo oraciones.csv procesado, limpiado y ordenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Funci√≥n para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar vi√±etas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (n√∫meros seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la l√≠nea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeraci√≥n de p√°gina (ejemplo: 'P√°gina 1', 'p√°g. 2', etc.)\n",
    "    text = re.sub(r'P√°gina \\d+', '', text)\n",
    "    text = re.sub(r'p√°g\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfab√©ticos ni num√©ricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar m√∫ltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", ordenar por ID y guardar\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no est√° vac√≠a\n",
    "                    if row['Oracion']:  \n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Ordenar las filas por ID (conversi√≥n a int para evitar errores de ordenaci√≥n)\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "\n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado, limpiado y ordenado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la funci√≥n\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CSV actualizado con √©xito. Se agregaron las columnas 'Temas Clave'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Cargar modelo de lenguaje en espa√±ol\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el CSV con las oraciones\n",
    "df = pd.read_csv(\"oraciones.csv\", sep=\";\")\n",
    "\n",
    "# Lista ampliada de temas clave\n",
    "temas_relevantes = {\n",
    "    \"econom√≠a\", \"educaci√≥n\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupci√≥n\", \"tecnolog√≠a\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"pol√≠tica\", \"desarrollo\", \"energ√≠a\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovaci√≥n\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversi√≥n\",\n",
    "    \"vivienda\", \"servicios p√∫blicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educaci√≥n superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalizaci√≥n\", \"gesti√≥n p√∫blica\", \"comercio\",\n",
    "    \"cambio clim√°tico\", \"energ√≠as renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud p√∫blica\", \"gobernanza\", \"justicia social\", \"igualdad de g√©nero\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestaci√≥n\", \"movilidad urbana\", \"biodiversidad\", \"educaci√≥n financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnol√≥gica\",\n",
    "    \"educaci√≥n digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnolog√≠a\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"econom√≠a circular\",\n",
    "    \"ciudades inteligentes\", \"protecci√≥n de datos\", \"energ√≠a solar\", \"transporte el√©ctrico\",\n",
    "    \"robotizaci√≥n\", \"computaci√≥n cu√°ntica\", \"espacio exterior\", \"protecci√≥n ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad el√©ctrica\", \"alimentos org√°nicos\", \"tecnolog√≠a educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas aut√≥nomos\",\n",
    "    \"tecnolog√≠a espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energ√≠a e√≥lica\", \"tecnolog√≠as disruptivas\", \"energ√≠a geot√©rmica\",\n",
    "    \"nanotecnolog√≠a\", \"microbioma\", \"bioeconom√≠a\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energ√≠a limpia\", \"criptomonedas\", \"miner√≠a digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educaci√≥n en l√≠nea\",\n",
    "    \"biomimicry\", \"ecoinnovaci√≥n\", \"simulaci√≥n computacional\", \"agricultura urbana\", \"cultivos inteligentes\"\n",
    "}\n",
    "\n",
    "# Funci√≥n para extraer solo los temas clave\n",
    "def extraer_temas_clave(texto):\n",
    "    if pd.isna(texto):  # Manejar valores nulos\n",
    "        return \"\"\n",
    "\n",
    "    oraciones = sent_tokenize(texto, language=\"spanish\")  # Dividir en oraciones\n",
    "    temas_detectados = set()\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        # Identificar temas clave dentro de la oraci√≥n\n",
    "        temas_detectados.update({tema for tema in temas_relevantes if tema in oracion.lower()})\n",
    "\n",
    "    # Retornar los temas clave detectados como una cadena separada por coma\n",
    "    return \", \".join(temas_detectados)\n",
    "\n",
    "# Aplicar la extracci√≥n de temas clave en cada fila del DataFrame\n",
    "df[\"Temas Clave\"] = df[\"Oracion\"].apply(extraer_temas_clave)\n",
    "\n",
    "# Guardar el nuevo CSV con la nueva columna 'Temas Clave' sin eliminar datos anteriores\n",
    "df.to_csv(\"oraciones_actualizado.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\" CSV actualizado con √©xito. Se agregaron las columnas 'Temas Clave'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,lemmatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stopwords en espa√±ol de NLTK\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Funci√≥n para lematizar y eliminar stopwords\n",
    "def lematizar_y_eliminar_stopwords(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Lematizar y eliminar stopwords\n",
    "    return ' '.join([token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words])\n",
    "\n",
    "# Limpiar contenido eliminando puntuaciones y n√∫meros\n",
    "def clean_content(texto):\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar puntuaciones\n",
    "    texto = re.sub(r'\\d+', '', texto)      # Eliminar n√∫meros\n",
    "    return texto.lower()\n",
    "\n",
    "# Funci√≥n para procesar el CSV\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: lematizar y eliminar stopwords\n",
    "                        row['Oracion'] = lematizar_y_eliminar_stopwords(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no est√° vac√≠a\n",
    "                    if row['Oracion']:  # Si no est√° vac√≠o o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion', 'Temas Clave'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la funci√≥n\n",
    "input_csv = 'oraciones_actualizado.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado como 'oraciones_procesadas_completo.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los archivos CSV asegurando el delimitador correcto\n",
    "df_oraciones = pd.read_csv('oraciones_procesadas.csv', delimiter=';')\n",
    "df_candidatos = pd.read_csv('candidatos.csv', delimiter=';')\n",
    "\n",
    "# Limpiar la columna 'ID' eliminando espacios y asegurando que solo contenga n√∫meros\n",
    "df_oraciones['ID'] = df_oraciones['ID'].astype(str).str.extract('(\\d+)').astype(float).astype('Int64')\n",
    "df_candidatos['ID'] = df_candidatos['ID'].astype(str).str.extract('(\\d+)').astype(float).astype('Int64')\n",
    "\n",
    "# Realizar la fusi√≥n de datos usando 'ID' como clave\n",
    "df_completo = df_oraciones.merge(df_candidatos, on='ID', how='left')\n",
    "\n",
    "# Guardar el nuevo CSV con los datos combinados\n",
    "df_completo.to_csv('oraciones_procesadas_completo.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Archivo combinado guardado como 'oraciones_procesadas_completo.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n",
      "Primeras filas de df:\n",
      "   ID Oracion_ID                                            Oracion  \\\n",
      "0   1          1  objetivo general alcanzar buen vivir democraci...   \n",
      "1   1          2                       objetivo espec√≠fico objetivo   \n",
      "2   1          3                       justicia buen vivir objetivo   \n",
      "3   1          4     justicia alcanzar paz seguridad orden objetivo   \n",
      "4   1          5  justicia bienestar econ√≥mico igualdad oportuni...   \n",
      "\n",
      "                               Temas Clave                      Partido  \\\n",
      "0  democracia, pol√≠tica, justicia, cultura  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "1                                      NaN  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "2                                 justicia  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "3                      justicia, seguridad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "4                       justicia, igualdad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "\n",
      "  CandidatoPresidente CandidatoVicePresidente ListaPolitica  \n",
      "0      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "1      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "2      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "3      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "4      LUISA GONZALEZ             DIEGO BORJA          5-33  \n",
      "Primeras filas de df2:\n",
      "    id  oracion_id                                   oracion_original  \\\n",
      "0  AG1           1  Andrea Gonz√°lez Nader, candidata presidencial ...   \n",
      "1  AG1           2  Propone una reducci√≥n del aparato estatal medi...   \n",
      "2  AG1           3  En materia de seguridad, destaca la necesidad ...   \n",
      "3  AG1           4  Adem√°s, promueve una reforma constitucional si...   \n",
      "4  AG1           5  En el √°mbito de salud, propone un enfoque prev...   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "0  andrea gonzalez nader candidata presidencial p...   \n",
      "1  propone una reduccion del aparato estatal medi...   \n",
      "2  en materia de seguridad destaca la necesidad d...   \n",
      "3  ademas promueve una reforma constitucional sin...   \n",
      "4  en el ambito de salud propone un enfoque preve...   \n",
      "\n",
      "                                oracion_sinStopWords       presidente  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...  ANDREA GONZALEZ   \n",
      "1  propone reduccion aparato estatal mediante eli...  ANDREA GONZALEZ   \n",
      "2  materia seguridad destaca necesidad retomar co...  ANDREA GONZALEZ   \n",
      "3  ademas promueve reforma constitucional recurri...  ANDREA GONZALEZ   \n",
      "4  ambito salud propone enfoque preventivo lugar ...  ANDREA GONZALEZ   \n",
      "\n",
      "  vicepresidente  lista                                  Partido  \n",
      "0   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "1   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "2   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "3   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "4   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO  \n",
      "Embeddings en df:\n",
      "                                             Oracion  \\\n",
      "0  objetivo general alcanzar buen vivir democraci...   \n",
      "1                       objetivo espec√≠fico objetivo   \n",
      "2                       justicia buen vivir objetivo   \n",
      "3     justicia alcanzar paz seguridad orden objetivo   \n",
      "4  justicia bienestar econ√≥mico igualdad oportuni...   \n",
      "\n",
      "                                   embedding_Oracion  \n",
      "0  [[-0.33315042, 0.08386034, 0.17690535, 0.11592...  \n",
      "1  [[-0.52293205, -0.009774199, 0.16899616, 0.168...  \n",
      "2  [[-0.5875125, -0.185553, -0.051314298, 0.23033...  \n",
      "3  [[-0.66177183, -0.045373067, 0.24473608, 0.280...  \n",
      "4  [[-0.40853977, -0.14116393, 0.25238565, 0.2702...  \n",
      "Embeddings en df2:\n",
      "                                oracion_sinStopWords  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...   \n",
      "1  propone reduccion aparato estatal mediante eli...   \n",
      "2  materia seguridad destaca necesidad retomar co...   \n",
      "3  ademas promueve reforma constitucional recurri...   \n",
      "4  ambito salud propone enfoque preventivo lugar ...   \n",
      "\n",
      "                      embedding_oracion_sinStopWords  \n",
      "0  [[-0.42224392, -0.14215767, 0.43130305, 0.2447...  \n",
      "1  [[-0.29042023, 0.16923054, 0.4194137, 0.332981...  \n",
      "2  [[-0.5043017, -0.0928126, 0.359641, 0.17502746...  \n",
      "3  [[-0.36289763, 0.026272643, 0.3643691, 0.25889...  \n",
      "4  [[-0.32233056, 0.081609026, 0.14803693, 0.3004...  \n",
      "Embeddings guardados en archivos .pkl.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# Detectar dispositivo: usa GPU si est√° disponible\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cargar los archivos CSV\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "df2 = pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "\n",
    "# Ver los primeros registros de ambos archivos para comprobar que se cargaron correctamente\n",
    "print(\"Primeras filas de df:\")\n",
    "print(df.head())\n",
    "print(\"Primeras filas de df2:\")\n",
    "print(df2.head())\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT preentrenado y mover el modelo al dispositivo (GPU o CPU)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased').to(device)\n",
    "\n",
    "# Funci√≥n para obtener el embedding de una oraci√≥n utilizando la GPU\n",
    "def obtener_embedding(texto):\n",
    "    # Asegurarse de que el texto sea una cadena\n",
    "    texto = str(texto)\n",
    "    # Tokenizar el texto y mover los tensores al dispositivo\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "    \n",
    "    # Obtener los embeddings de BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Tomar el embedding del token [CLS] (primer token) y moverlo a la CPU para convertirlo en numpy\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    return embedding\n",
    "\n",
    "# Generar embeddings para las oraciones en el primer DataFrame (df)\n",
    "df['embedding_Oracion'] = df['Oracion'].apply(lambda x: obtener_embedding(x))\n",
    "# Generar embeddings para las oraciones en el segundo DataFrame (df2)\n",
    "df2['embedding_oracion_sinStopWords'] = df2['oracion_sinStopWords'].apply(lambda x: obtener_embedding(x))\n",
    "\n",
    "# Ver los primeros resultados con los embeddings generados\n",
    "print(\"Embeddings en df:\")\n",
    "print(df[['Oracion', 'embedding_Oracion']].head())\n",
    "\n",
    "print(\"Embeddings en df2:\")\n",
    "print(df2[['oracion_sinStopWords', 'embedding_oracion_sinStopWords']].head())\n",
    "\n",
    "# Guardar los DataFrames con los embeddings generados en formato .pkl\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)\n",
    "\n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'wb') as f:\n",
    "    pickle.dump(df2, f)\n",
    "\n",
    "print(\"Embeddings guardados en archivos .pkl.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df962099739240fa91048bf43a4bbb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n",
      "{'ID': 4, 'Oracion_ID': '993', 'Oracion': 'eficiencia energ√©tico implementar pol√≠tica eficiencia energ√©tico sector clave industria transporte construcci√≥n promover uso tecnolog√≠a reducir consumo energ√≠a', 'Temas Clave': 'energ√≠a, transporte, ciencia, tecnolog√≠a, pol√≠tica, industria', 'Partido': 'MOVIMIENTO CENTRO DEMOCR√ÅTICO', 'CandidatoPresidente': 'JIMMY JAIRALA VALLAZZA', 'CandidatoVicePresidente': 'LUCIA VALLECILLA SUAREZ', 'ListaPolitica': '1', 'Distancia': 0.33702278}\n",
      "{'ID': 14, 'Oracion_ID': '811', 'Oracion': 'adem√°s realizar campa√±a nacional eficiencia ahorro energ√©tico √©nfasis sector comercial residencial industrial institucional', 'Temas Clave': 'industria, ciencia', 'Partido': 'PARTIDO AVANZA', 'CandidatoPresidente': 'LUIS FELIPE TILLERIA', 'CandidatoVicePresidente': 'KARLA PAULINA ROSERO', 'ListaPolitica': '8', 'Distancia': 0.38165388}\n",
      "{'ID': 14, 'Oracion_ID': '827', 'Oracion': 'campa√±a nacional eficiencia ahorro energ√©tico promover pr√°ctica eficiencia ahorro energ√©tico sector', 'Temas Clave': 'ciencia', 'Partido': 'PARTIDO AVANZA', 'CandidatoPresidente': 'LUIS FELIPE TILLERIA', 'CandidatoVicePresidente': 'KARLA PAULINA ROSERO', 'ListaPolitica': '8', 'Distancia': 0.4163392}\n",
      "{'ID': 8, 'Oracion_ID': '433', 'Oracion': 'impulso inversi√≥n privado gobierno deber crear entorno atractivo inversi√≥n privado sector energ√©tico', 'Temas Clave': 'gobierno, inversi√≥n', 'Partido': 'MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID', 'CandidatoPresidente': 'VICTOR ARAUS', 'CandidatoVicePresidente': 'CRISTINA CARRERA', 'ListaPolitica': '4', 'Distancia': 0.42793632}\n",
      "{'ID': 1, 'Oracion_ID': '533', 'Oracion': 'intervenir vivienda promover eficiencia energ√©tico', 'Temas Clave': 'vivienda, ciencia', 'Partido': 'REVOLUCI√ìN CIUDADANA - RETO', 'CandidatoPresidente': 'LUISA GONZALEZ', 'CandidatoVicePresidente': 'DIEGO BORJA', 'ListaPolitica': '5-33', 'Distancia': 0.42992368}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar los archivos CSV\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "df2=pd.read_csv('./data/interview/elecciones_oraciones.csv', sep=\",\")\n",
    "\n",
    "# Inicializar el modelo BERT y moverlo a GPU si est√° disponible\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un √≠ndice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Guardar los embeddings e √≠ndice FAISS\n",
    "np.save('embeddings.npy', embeddings)\n",
    "faiss.write_index(index, 'faiss_index.index')\n",
    "\n",
    "# Guardar datos procesados en un archivo pickle\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n",
    "\n",
    "# Cargar spaCy para espa√±ol\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def query_faiss(query, top_k=5):\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    \n",
    "    with open('sentences.pkl', 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        result = sentences[I[0][i]]\n",
    "        results.append({\n",
    "            'ID': result['ID'],\n",
    "            'Oracion_ID':result['Oracion_ID'],\n",
    "            'Oracion': result['Oracion'],\n",
    "            'Temas Clave': result['Temas Clave'],\n",
    "            'Partido': result['Partido'],\n",
    "            'CandidatoPresidente': result['CandidatoPresidente'],\n",
    "            'CandidatoVicePresidente': result['CandidatoVicePresidente'],\n",
    "            'ListaPolitica': result['ListaPolitica'],\n",
    "            'Distancia': D[0][i]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¬øC√≥mo mejorar la eficiencia energ√©tica en la industria?\"\n",
    "results = query_faiss(query)\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primeras filas del primer DataFrame:\n",
      "   ID Oracion_ID                                            Oracion  \\\n",
      "0   1          1  objetivo general alcanzar buen vivir democraci...   \n",
      "1   1          2                       objetivo espec√≠fico objetivo   \n",
      "2   1          3                       justicia buen vivir objetivo   \n",
      "3   1          4     justicia alcanzar paz seguridad orden objetivo   \n",
      "4   1          5  justicia bienestar econ√≥mico igualdad oportuni...   \n",
      "\n",
      "                               Temas Clave                      Partido  \\\n",
      "0  democracia, pol√≠tica, justicia, cultura  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "1                                      NaN  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "2                                 justicia  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "3                      justicia, seguridad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "4                       justicia, igualdad  REVOLUCI√ìN CIUDADANA - RETO   \n",
      "\n",
      "  CandidatoPresidente CandidatoVicePresidente ListaPolitica  \\\n",
      "0      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "1      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "2      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "3      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "4      LUISA GONZALEZ             DIEGO BORJA          5-33   \n",
      "\n",
      "                                   embedding_Oracion  \n",
      "0  [[-0.33315042, 0.08386034, 0.17690535, 0.11592...  \n",
      "1  [[-0.52293205, -0.009774199, 0.16899616, 0.168...  \n",
      "2  [[-0.5875125, -0.185553, -0.051314298, 0.23033...  \n",
      "3  [[-0.66177183, -0.045373067, 0.24473608, 0.280...  \n",
      "4  [[-0.40853977, -0.14116393, 0.25238565, 0.2702...  \n",
      "\n",
      "Primeras filas del segundo DataFrame:\n",
      "    id  oracion_id                                   oracion_original  \\\n",
      "0  AG1           1  Andrea Gonz√°lez Nader, candidata presidencial ...   \n",
      "1  AG1           2  Propone una reducci√≥n del aparato estatal medi...   \n",
      "2  AG1           3  En materia de seguridad, destaca la necesidad ...   \n",
      "3  AG1           4  Adem√°s, promueve una reforma constitucional si...   \n",
      "4  AG1           5  En el √°mbito de salud, propone un enfoque prev...   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "0  andrea gonzalez nader candidata presidencial p...   \n",
      "1  propone una reduccion del aparato estatal medi...   \n",
      "2  en materia de seguridad destaca la necesidad d...   \n",
      "3  ademas promueve una reforma constitucional sin...   \n",
      "4  en el ambito de salud propone un enfoque preve...   \n",
      "\n",
      "                                oracion_sinStopWords       presidente  \\\n",
      "0  andrea gonzalez nader candidata presidencial s...  ANDREA GONZALEZ   \n",
      "1  propone reduccion aparato estatal mediante eli...  ANDREA GONZALEZ   \n",
      "2  materia seguridad destaca necesidad retomar co...  ANDREA GONZALEZ   \n",
      "3  ademas promueve reforma constitucional recurri...  ANDREA GONZALEZ   \n",
      "4  ambito salud propone enfoque preventivo lugar ...  ANDREA GONZALEZ   \n",
      "\n",
      "  vicepresidente  lista                                  Partido  \\\n",
      "0   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "1   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "2   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "3   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "4   GALO MONCAYO    3.0  PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO   \n",
      "\n",
      "                      embedding_oracion_sinStopWords  \n",
      "0  [[-0.42224392, -0.14215767, 0.43130305, 0.2447...  \n",
      "1  [[-0.29042023, 0.16923054, 0.4194137, 0.332981...  \n",
      "2  [[-0.5043017, -0.0928126, 0.359641, 0.17502746...  \n",
      "3  [[-0.36289763, 0.026272643, 0.3643691, 0.25889...  \n",
      "4  [[-0.32233056, 0.081609026, 0.14803693, 0.3004...  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Cargar el DataFrame del primer CSV (por ejemplo, que contiene la columna 'embedding_Oracion')\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "\n",
    "# Cargar el DataFrame del segundo CSV (por ejemplo, que contiene la columna 'embedding_oracion_sinStopWords')\n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# Verificar la carga\n",
    "print(\"Primeras filas del primer DataFrame:\")\n",
    "print(df1.head())\n",
    "print(\"\\nPrimeras filas del segundo DataFrame:\")\n",
    "print(df2.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El √≠ndice FAISS del primer DataFrame contiene 13759 vectores.\n",
      "El √≠ndice FAISS del segundo DataFrame contiene 7753 vectores.\n",
      "√çndice FAISS del primer DataFrame guardado en 'faiss_index_oraciones_procesadas.index'.\n",
      "√çndice FAISS del segundo DataFrame guardado en 'faiss_index_elecciones_oraciones.index'.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# --- Para el primer DataFrame ---\n",
    "# Extraer y apilar los embeddings\n",
    "embeddings1 = np.vstack(df1['embedding_Oracion'].values).astype('float32')\n",
    "dim1 = embeddings1.shape[1]\n",
    "\n",
    "# Crear el √≠ndice FAISS (usamos IndexFlatL2 para b√∫squeda por distancia euclidiana)\n",
    "index1 = faiss.IndexFlatL2(dim1)\n",
    "index1.add(embeddings1)\n",
    "print(f\"El √≠ndice FAISS del primer DataFrame contiene {index1.ntotal} vectores.\")\n",
    "\n",
    "# --- Para el segundo DataFrame ---\n",
    "# Extraer y apilar los embeddings\n",
    "embeddings2 = np.vstack(df2['embedding_oracion_sinStopWords'].values).astype('float32')\n",
    "dim2 = embeddings2.shape[1]\n",
    "\n",
    "# Crear el √≠ndice FAISS para el segundo conjunto de embeddings\n",
    "index2 = faiss.IndexFlatL2(dim2)\n",
    "index2.add(embeddings2)\n",
    "print(f\"El √≠ndice FAISS del segundo DataFrame contiene {index2.ntotal} vectores.\")\n",
    "\n",
    "# Guardar el √≠ndice del primer DataFrame\n",
    "faiss.write_index(index1, 'faiss_index_oraciones_procesadas.index')\n",
    "print(\"√çndice FAISS del primer DataFrame guardado en 'faiss_index_oraciones_procesadas.index'.\")\n",
    "\n",
    "# Guardar el √≠ndice del segundo DataFrame\n",
    "faiss.write_index(index2, 'faiss_index_elecciones_oraciones.index')\n",
    "print(\"√çndice FAISS del segundo DataFrame guardado en 'faiss_index_elecciones_oraciones.index'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se agregaron 13759 vectores al √≠ndice FAISS.\n",
      "Se agregaron 7753 vectores al √≠ndice FAISS.\n",
      "\n",
      "--- Consulta en el primer √≠ndice (df1) ---\n",
      "\n",
      "Consulta: \"eduacion\"\n",
      "Indices encontrados: [ 4651  4647  8323  8647 12534]\n",
      "Distancias: [12.340698 13.612962 13.612962 13.612962 13.612962]\n",
      "\n",
      "Resultados:\n",
      "       ID Oracion_ID         Oracion Temas Clave  \\\n",
      "4651    3       1499  concienciaci√≥n     ciencia   \n",
      "4647    3       1495       educaci√≥n   educaci√≥n   \n",
      "8323    7        510       educaci√≥n   educaci√≥n   \n",
      "8647    7        938       educaci√≥n   educaci√≥n   \n",
      "12534  14        586       educaci√≥n   educaci√≥n   \n",
      "\n",
      "                                                 Partido  \\\n",
      "4651                       PARTIDO IZQUIERDA DEMOCR√ÅTICA   \n",
      "4647                       PARTIDO IZQUIERDA DEMOCR√ÅTICA   \n",
      "8323   MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPEND...   \n",
      "8647   MOVIMIENTO AMIGO, ACCI√ìN MOVILIZADORA INDEPEND...   \n",
      "12534                                     PARTIDO AVANZA   \n",
      "\n",
      "        CandidatoPresidente   CandidatoVicePresidente ListaPolitica  \\\n",
      "4651       CARLOS RABASCALL  ALEJANDRA RIVAS MANTILLA            12   \n",
      "4647       CARLOS RABASCALL  ALEJANDRA RIVAS MANTILLA            12   \n",
      "8323        JUAN IVAN CUEVA            CRISTINA REYES            16   \n",
      "8647        JUAN IVAN CUEVA            CRISTINA REYES            16   \n",
      "12534  LUIS FELIPE TILLERIA      KARLA PAULINA ROSERO             8   \n",
      "\n",
      "                                       embedding_Oracion  \n",
      "4651   [[-0.37460178, -0.08777584, -0.20984206, -0.14...  \n",
      "4647   [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "8323   [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "8647   [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "12534  [[-0.2738171, 0.007819946, 0.054867085, -0.142...  \n",
      "\n",
      "--- Consulta en el segundo √≠ndice (df2) ---\n",
      "\n",
      "Consulta: \"bajar iva.\"\n",
      "Indices encontrados: [1102 4448 3400 4148 5540]\n",
      "Distancias: [40.086105 40.801044 42.0157   43.03201  43.514687]\n",
      "\n",
      "Resultados:\n",
      "       id  oracion_id                                 oracion_original  \\\n",
      "1102  FT2          69                      Pero bueno, fuera de broma.   \n",
      "4448  LT3         256        La palabra en ingl√©s es rogue, R-O-G-Q-E.   \n",
      "3400  JC3          83         ¬øElla podr√≠a convertirse en un aminista?   \n",
      "4148  LT2          81                       Vamos a la contra r√©plica.   \n",
      "5540  DN1         128  Y nosotros tenemos que estar abiertos para eso.   \n",
      "\n",
      "                                      oracion_limpia  \\\n",
      "1102                       pero bueno fuera de broma   \n",
      "4448         la palabra en ingles es rogue r o g q e   \n",
      "3400          ella podria convertirse en un aminista   \n",
      "4148                       vamos a la contra replica   \n",
      "5540  y nosotros tenemos que estar abiertos para eso   \n",
      "\n",
      "             oracion_sinStopWords          presidente    vicepresidente  \\\n",
      "1102                  bueno broma  FRANCESCO TABACCHI  BLANCA SACANCELA   \n",
      "4448   palabra ingles rogue r g q                 NaN               NaN   \n",
      "3400  podria convertirse aminista                 NaN               NaN   \n",
      "4148                vamos replica                 NaN               NaN   \n",
      "5540                     abiertos   DANIEL NOBOA AZIN  MARIA JOSE PINTO   \n",
      "\n",
      "      lista                                      Partido  \\\n",
      "1102   21.0       MOVIMIENTO CREO, CREANDO OPORTUNIDADES   \n",
      "4448    NaN                                          NaN   \n",
      "3400    NaN                                          NaN   \n",
      "4148    NaN                                          NaN   \n",
      "5540    7.0  MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN   \n",
      "\n",
      "                         embedding_oracion_sinStopWords  \n",
      "1102  [[-0.5601457, -0.07494587, -0.04576185, -0.137...  \n",
      "4448  [[-0.56483746, -0.007231636, 0.07722815, -0.11...  \n",
      "3400  [[-0.6505401, -0.12425378, -0.21291989, -0.107...  \n",
      "4148  [[-0.41923407, -0.108675756, -0.20688075, 0.03...  \n",
      "5540  [[-0.5437605, 0.036334, -0.060376026, -0.25117...  \n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT (usa el mismo que usaste para generar los embeddings)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def obtener_embedding(texto):\n",
    "    \"\"\"\n",
    "    Dada una cadena de texto, devuelve el embedding correspondiente usando BERT.\n",
    "    Se utiliza el embedding del token [CLS] como representaci√≥n.\n",
    "    \"\"\"\n",
    "    texto = str(texto)  # asegurar que sea string\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Se toma el embedding del token [CLS] (posici√≥n 0) y se aplana a 1D (768,)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "    return embedding\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Cargar los DataFrames con embeddings desde los archivos .pkl\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ejemplo para el primer CSV (ajusta el nombre del archivo seg√∫n corresponda)\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "    \n",
    "# Ejemplo para el segundo CSV\n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Reconstruir los √≠ndices FAISS a partir de los embeddings almacenados en los DataFrames\n",
    "# ---------------------------------------------------------------------------\n",
    "def construir_index_faiss(df, embedding_col):\n",
    "    \"\"\"\n",
    "    Dado un DataFrame y el nombre de la columna que contiene los embeddings,\n",
    "    crea y devuelve un √≠ndice FAISS usando IndexFlatL2.\n",
    "    \"\"\"\n",
    "    # Extraer y apilar los embeddings en una matriz NumPy de tipo float32\n",
    "    embeddings_matrix = np.vstack(df[embedding_col].values).astype('float32')\n",
    "    dim = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings_matrix)\n",
    "    print(f\"Se agregaron {index.ntotal} vectores al √≠ndice FAISS.\")\n",
    "    return index\n",
    "\n",
    "# Para el primer DataFrame, asumimos que la columna de embeddings se llama 'embedding_Oracion'\n",
    "index1 = construir_index_faiss(df1, 'embedding_Oracion')\n",
    "\n",
    "# Para el segundo DataFrame, asumimos que la columna de embeddings se llama 'embedding_oracion_sinStopWords'\n",
    "index2 = construir_index_faiss(df2, 'embedding_oracion_sinStopWords')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Funci√≥n para hacer consultas en un √≠ndice FAISS\n",
    "# ---------------------------------------------------------------------------\n",
    "def hacer_consulta(index, df, query_text, k=5):\n",
    "    \"\"\"\n",
    "    Dada una consulta en forma de texto:\n",
    "      - Calcula su embedding.\n",
    "      - Busca los k vecinos m√°s cercanos en el √≠ndice FAISS.\n",
    "      - Recupera y muestra la metadata de los registros encontrados.\n",
    "    \n",
    "    Par√°metros:\n",
    "      index: √çndice FAISS.\n",
    "      df: DataFrame original con la metadata, donde el orden coincide con el √≠ndice.\n",
    "      query_text: Texto de consulta.\n",
    "      k: N√∫mero de vecinos a recuperar.\n",
    "    \"\"\"\n",
    "    # Obtener el embedding de la consulta y asegurarse de que sea float32\n",
    "    embedding_query = obtener_embedding(query_text).astype('float32')\n",
    "    # Expandir dimensiones para que sea (1, dim)\n",
    "    query_vector = np.expand_dims(embedding_query, axis=0)\n",
    "    \n",
    "    # Buscar en el √≠ndice\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # Mostrar resultados\n",
    "    print(f\"\\nConsulta: \\\"{query_text}\\\"\")\n",
    "    print(\"Indices encontrados:\", indices[0])\n",
    "    print(\"Distancias:\", distances[0])\n",
    "    print(\"\\nResultados:\")\n",
    "    # Se asume que el DataFrame tiene la metadata deseada en las columnas originales.\n",
    "    # Por ejemplo, para df1 se podr√≠an mostrar 'Oracion', 'CandidatoPresidente', etc.\n",
    "    resultados = df.iloc[indices[0]]\n",
    "    print(resultados)\n",
    "    return distances, indices\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Ejemplo de consulta\n",
    "# ---------------------------------------------------------------------------\n",
    "# Consulta en el primer √≠ndice (df1) usando la columna 'embedding_Oracion'\n",
    "query_text1 = \"eduacion\"\n",
    "print(\"\\n--- Consulta en el primer √≠ndice (df1) ---\")\n",
    "_ = hacer_consulta(index1, df1, query_text1, k=5)\n",
    "\n",
    "# Consulta en el segundo √≠ndice (df2) usando la columna 'embedding_oracion_sinStopWords'\n",
    "query_text2 = \"bajar iva.\"\n",
    "print(\"\\n--- Consulta en el segundo √≠ndice (df2) ---\")\n",
    "_ = hacer_consulta(index2, df2, query_text2, k=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Consulta ---\n",
      "Distancia obtenida: 32.213364\n",
      "√çndice en el √≠ndice FAISS: 18\n",
      "Candidato encontrado: {'nombre': 'ANDREA GONZALEZ', 'tipo': 'presidente', 'whois': 'Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.'}\n",
      "Resultado: Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Andrea Gonz√°lez Nader es una activista medioambiental y pol√≠tica ecuatoriana de 37 a√±os, originaria de Guayaquil. Ha trabajado como profesora en instituciones educativas como los colegios Delta-Torremar, Cruz del Sur y la Unidad Educativa Biling√ºe Delta entre 2007 y 2015. Adem√°s, ha colaborado con organizaciones medioambientales como la Fundaci√≥n La Iguana y ha liderado proyectos de paisajismo sostenible y manejo integral de desechos. En 2023, fue candidata a la vicepresidencia de la Rep√∫blica junto al periodista Fernando Villavicencio, y posteriormente acompa√±√≥ al periodista Christian Zurita en el mismo proceso electoral. En agosto de 2024, fue proclamada candidata presidencial por el Partido Sociedad Patri√≥tica (PSP) para las elecciones de 2025.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# Cargar el archivo CSV con el delimitador ';'\n",
    "df = pd.read_csv('candidatos.csv', delimiter=';')\n",
    "\n",
    "# Cargar el modelo BERT y su tokenizer (en este ejemplo se usa 'bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Funci√≥n para normalizar texto: convierte a min√∫sculas y elimina acentos\n",
    "def normalizar_texto(texto):\n",
    "    texto = texto.lower()\n",
    "    texto = unicodedata.normalize('NFD', texto)\n",
    "    texto = ''.join([c for c in texto if unicodedata.category(c) != 'Mn'])\n",
    "    return texto\n",
    "\n",
    "# Funci√≥n para obtener el embedding de un texto usando BERT.\n",
    "# Se utiliza el promedio de la √∫ltima capa (puedes ajustar esta estrategia si lo deseas)\n",
    "def obtener_embedding(texto):\n",
    "    inputs = tokenizer(texto, return_tensors='pt', padding=True, truncation=True, max_length=64)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Promediamos los embeddings de la secuencia (alternativamente se podr√≠a usar el token [CLS])\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Construir la lista de entradas de candidatos a partir de las dos columnas:\n",
    "# Se generan entradas separadas para cada candidato, indicando si es presidente o vice,\n",
    "# y se asocia el correspondiente valor de \"Whois\"\n",
    "candidatos = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Si existe un candidato presidente, se agrega una entrada\n",
    "    if pd.notna(row['CandidatoPresidente']):\n",
    "        entry = {\n",
    "            'nombre': row['CandidatoPresidente'],\n",
    "            'tipo': 'presidente',\n",
    "            'whois': row['WhoisPresident']\n",
    "        }\n",
    "        candidatos.append(entry)\n",
    "    # Si existe un candidato vicepresidente, se agrega otra entrada\n",
    "    if pd.notna(row['CandidatoVicePresidente']):\n",
    "        entry = {\n",
    "            'nombre': row['CandidatoVicePresidente'],\n",
    "            'tipo': 'vice',\n",
    "            'whois': row['WhoisVice']\n",
    "        }\n",
    "        candidatos.append(entry)\n",
    "\n",
    "# Generar los embeddings para cada candidato (usando el nombre normalizado)\n",
    "embeddings = []\n",
    "for entry in candidatos:\n",
    "    nombre_normalizado = normalizar_texto(entry['nombre'])\n",
    "    emb = obtener_embedding(nombre_normalizado)\n",
    "    embeddings.append(emb)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear el √≠ndice FAISS utilizando la m√©trica L2 para b√∫squedas por similitud\n",
    "d = embeddings.shape[1]  # Dimensionalidad del embedding\n",
    "index = faiss.IndexFlatL2(d)\n",
    "index.add(embeddings)\n",
    "\n",
    "def buscar_candidato_por_similitud(query_text):\n",
    "    # Convertir la consulta a min√∫sculas y remover palabras comunes (\"qui√©n\", \"quien\", \"es\")\n",
    "    query_lower = query_text.lower()\n",
    "    for palabra in [\"qui√©n\", \"quien\", \"es\"]:\n",
    "        query_lower = query_lower.replace(palabra, \"\")\n",
    "    candidate_query = query_lower.strip()\n",
    "    \n",
    "    # Normalizar el texto extra√≠do\n",
    "    normalized_candidate_query = normalizar_texto(candidate_query)\n",
    "    \n",
    "    # Obtener el embedding de la consulta\n",
    "    embedding_query = obtener_embedding(normalized_candidate_query)\n",
    "    embedding_query = np.array([embedding_query]).astype('float32')\n",
    "    \n",
    "    # Buscar el candidato m√°s cercano en el √≠ndice FAISS\n",
    "    D, I = index.search(embedding_query, 1)\n",
    "    \n",
    "    # Mostrar informaci√≥n de depuraci√≥n\n",
    "    print(\"Distancia obtenida:\", D[0][0])\n",
    "    print(\"√çndice en el √≠ndice FAISS:\", I[0][0])\n",
    "    \n",
    "    # Obtener el candidato encontrado\n",
    "    candidato_encontrado = candidatos[I[0][0]]\n",
    "    print(\"Candidato encontrado:\", candidato_encontrado)\n",
    "    return candidato_encontrado['whois']\n",
    "\n",
    "# Funci√≥n principal para procesar la consulta\n",
    "def hacer_consulta(query_text):\n",
    "    if \"qui√©n\" in query_text.lower() or \"quien\" in query_text.lower():\n",
    "        resultado = buscar_candidato_por_similitud(query_text)  # Llamada sin 'umbral'\n",
    "        print(\"Resultado:\", resultado)\n",
    "        return resultado\n",
    "    else:\n",
    "        print(\"Consulta no v√°lida.\")\n",
    "        return \"Consulta no v√°lida.\"\n",
    "\n",
    "# Ejemplo de consulta\n",
    "query_text = \"qui√©n es CARLOS \"\n",
    "print(\"\\n--- Consulta ---\")\n",
    "hacer_consulta(query_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings guardados en 'candidatos_embeddings.pkl'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import pickle\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv(\"candidatos.csv\", delimiter=\";\")\n",
    "\n",
    "# Inicializar el tokenizador y el modelo de BERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Funci√≥n para obtener embeddings de BERT\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    \n",
    "    # Obtener las salidas del modelo\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Tomar la representaci√≥n del token [CLS] (primer token) como el embedding del texto\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].squeeze()\n",
    "    \n",
    "    return embedding.numpy()\n",
    "\n",
    "# Generar embeddings para las columnas de CandidatoPresidente y CandidatoVicePresidente\n",
    "presidente_embeddings = df[\"CandidatoPresidente\"].apply(get_bert_embeddings)\n",
    "vicepresidente_embeddings = df[\"CandidatoVicePresidente\"].apply(get_bert_embeddings)\n",
    "\n",
    "# Guardar los embeddings en un archivo pickle\n",
    "with open(\"candidatos_embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\n",
    "        \"presidente_embeddings\": presidente_embeddings.tolist(),\n",
    "        \"vicepresidente_embeddings\": vicepresidente_embeddings.tolist()\n",
    "    }, f)\n",
    "\n",
    "print(\"Embeddings guardados en 'candidatos_embeddings.pkl'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consultando: QUI√âN ES LUISA GONZALEZ?\n",
      "Nombre extra√≠do para b√∫squeda: ES LUISA GONZALEZ\n",
      "Resultado: Candidato Presidente: LUISA GONZALEZ (Distancia: 56.31492614746094)\n",
      "Quien es: LUISA GONZALEZ nacida en Quito en 1977, Luisa Gonz√°lez se considera manabita, ya que creci√≥ en la parroquia Canuto de Chone, Manab√≠. Es abogada y ha desempe√±ado diversos cargos en la funci√≥n p√∫blica durante el gobierno de Rafael Correa. Entre sus roles m√°s destacados se incluyen: Asesora de la Secretar√≠a de Comunicaci√≥n e Informaci√≥n en 2008.Coordinadora General de Recursos Humanos en la Superintendencia de Compa√±√≠as.C√≥nsul General de Ecuador en Madrid y Alicante.Viceministra de Gesti√≥n Tur√≠stica en el Ministerio de Turismo.Secretaria General de la Presidencia.En 2021, fue elegida asamble√≠sta por la provincia de Manab√≠. Durante su gesti√≥n, se opuso a la despenalizaci√≥n del aborto en casos de violaci√≥n y a la Ley de Salud e Higiene Menstrual. En 2023, fue candidata presidencial, pero perdi√≥ ante Daniel Noboa. En 2024, fue designada nuevamente como candidata presidencial por la Revoluci√≥n Ciudadana para las elecciones de 2025.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re  # Para usar expresiones regulares y limpiar la consulta\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Cargar los datos desde el CSV\n",
    "df = pd.read_csv(\"candidatos.csv\", delimiter=\";\")\n",
    "\n",
    "# Cargar el modelo BERT y el tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Funci√≥n para obtener los embeddings de BERT\n",
    "def get_bert_embeddings(texto):\n",
    "    # Tokenizar el texto\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    \n",
    "    # Obtener la salida del modelo BERT\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Obtener el embedding correspondiente a la √∫ltima capa (pooling)\n",
    "    embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Funci√≥n para normalizar los embeddings\n",
    "def normalizar_embeddings(embeddings):\n",
    "    return embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "\n",
    "# Suponiendo que ya tienes las funciones para obtener los embeddings\n",
    "presidente_embeddings = np.array([get_bert_embeddings(nombre).flatten() for nombre in df[\"CandidatoPresidente\"]])\n",
    "vicepresidente_embeddings = np.array([get_bert_embeddings(nombre).flatten() for nombre in df[\"CandidatoVicePresidente\"]])\n",
    "\n",
    "# Normalizar los embeddings\n",
    "presidente_embeddings = normalizar_embeddings(presidente_embeddings)\n",
    "vicepresidente_embeddings = normalizar_embeddings(vicepresidente_embeddings)\n",
    "\n",
    "# Concatenar los embeddings de presidente y vicepresidente\n",
    "all_embeddings = np.concatenate([presidente_embeddings, vicepresidente_embeddings], axis=0)\n",
    "\n",
    "# Crear un √≠ndice FAISS\n",
    "dimension = all_embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(all_embeddings.astype(np.float32))\n",
    "\n",
    "# Funci√≥n de b√∫squeda y verificaci√≥n de \"qui√©n\" + nombre\n",
    "def buscar_similares(nuevo_texto, k=1):  # Solo devolver el top 1\n",
    "    nuevo_texto = nuevo_texto.upper()  # Convertir la consulta a may√∫sculas\n",
    "    print(f\"Consultando: {nuevo_texto}\")\n",
    "    \n",
    "    # Comprobar si la consulta contiene \"qui√©n\" y un nombre\n",
    "    if \"QUI√âN\" in nuevo_texto:\n",
    "        # Limpiar la consulta para extraer solo el nombre del candidato\n",
    "        nombre = re.sub(r'[^A-Za-z\\s]', '', nuevo_texto.replace(\"QUI√âN\", \"\").strip())  # Remover caracteres no alfab√©ticos\n",
    "        nombre = nombre.strip()  # Eliminar espacios adicionales\n",
    "        print(f\"Nombre extra√≠do para b√∫squeda: {nombre}\")\n",
    "        \n",
    "        # Realizar b√∫squeda para presidente y vicepresidente\n",
    "        # Primero buscamos el presidente\n",
    "        distancias_presidente, indices_presidente = index.search(get_bert_embeddings(nombre).reshape(1, -1).astype(np.float32), k)\n",
    "        distancias_presidente, indices_presidente = zip(*sorted(zip(distancias_presidente[0], indices_presidente[0]), key=lambda x: x[0]))\n",
    "\n",
    "        # Si encontramos una coincidencia con el presidente, lo devolvemos\n",
    "        for i, (distancia, indice) in enumerate(zip(distancias_presidente, indices_presidente)):\n",
    "            if indice < len(presidente_embeddings):  # Si es presidente\n",
    "                candidato = df['CandidatoPresidente'].iloc[indice]\n",
    "                print(f\"Resultado: Candidato Presidente: {candidato} (Distancia: {distancia})\")\n",
    "                print(f\"Quien es: {df['WhoisPresident'].iloc[indice]}\")\n",
    "                return\n",
    "            else:\n",
    "                # Si no es presidente, lo buscamos en vicepresidente\n",
    "                indice_vice = indice - len(presidente_embeddings)\n",
    "                candidato = df['CandidatoVicePresidente'].iloc[indice_vice]\n",
    "                print(f\"Resultado: Candidato Vicepresidente: {candidato} (Distancia: {distancia})\")\n",
    "                print(f\"Quien es: {df['WhoisVice'].iloc[indice_vice]}\")\n",
    "                return\n",
    "    else:\n",
    "        print(\"No se encontr√≥ la palabra 'qui√©n' en la consulta.\")\n",
    "\n",
    "# Ejemplo de consulta con \"qui√©n\"\n",
    "consulta = \"Qui√©n es LUISA GONZALEZ?\"\n",
    "buscar_similares(consulta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir la cantidad total de elementos en el √≠ndice FAISS\n",
    "print(\"Total de elementos en el √≠ndice FAISS:\", index.ntotal)\n",
    "\n",
    "# Iterar sobre cada elemento en el √≠ndice y mostrar su contenido\n",
    "for i in range(index.ntotal):\n",
    "    # Reconstruir el vector almacenado en el √≠ndice para el √≠tem i\n",
    "    vector_reconstruido = index.reconstruct(i)\n",
    "    \n",
    "    # Mostrar el √≠ndice, la informaci√≥n del candidato y (opcionalmente) el vector\n",
    "    print(f\"\\n√çndice {i}:\")\n",
    "    print(\"Informaci√≥n del candidato:\", candidatos[i])\n",
    "    print(\"Embedding (vector):\", vector_reconstruido)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se agregaron 13759 vectores al √≠ndice FAISS.\n",
      "Se agregaron 7753 vectores al √≠ndice FAISS.\n",
      "\n",
      "--- Realizando consulta en ambos √≠ndices ---\n",
      "Consulta preprocesada: bajar ivo fortalecer economia\n",
      "\n",
      "Prompt para Ollama:\n",
      "Esto es lo que he encontrado.:\n",
      "\n",
      "oracion_plan: econom√≠a haber ser secuestrar oligarca banquero, Temas clave: econom√≠a, Partido: MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK, Candidato Presidente: 18, Candidato Vicepresidente: LEONIDAS IZA\n",
      "oracion_entrevista: Conozco... ¬øY alguien lidiar√° con usted?, Partido: PARTIDO SOCIAL CRISTIANO, Candidato Presidente: HENRY KRONFLE KOZHAYA, Candidato Vicepresidente: DALLYANA PASSAILAIGUE\n",
      "oracion_plan: comprometer cobrar impuesto grande deudor evasor, Temas clave: nan, Partido: PARTIDO SOCIALISTA ECUATORIANO, Candidato Presidente: 17, Candidato Vicepresidente: PEDRO GRANJA\n",
      "oracion_entrevista: Pero ya no pueden controlarlo ni robar., Partido: nan, Candidato Presidente: nan, Candidato Vicepresidente: nan\n",
      "oracion_plan: aquel afectar econom√≠a familia ecuatoriano, Temas clave: econom√≠a, Partido: PARTIDO UNIDAD POPULAR, Candidato Presidente: 2, Candidato Vicepresidente: JORGE ESCALA\n",
      "oracion_plan: restaurar ecosistema vital manglar, Temas clave: nan, Partido: MOVIMIENTO CREO, CREANDO OPORTUNIDADES, Candidato Presidente: 21, Candidato Vicepresidente: FRANCESCO TABACCHI\n",
      "oracion_entrevista: Listo hay que seguir manteniendo, hay que fortalecerla hay que garantizarla., Partido: MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK, Candidato Presidente: LEONIDAS IZA, Candidato Vicepresidente: KATIUSKA MOLINA\n",
      "oracion_plan: educar concientizar sociedad, Temas clave: nan, Partido: REVOLUCI√ìN CIUDADANA - RETO, Candidato Presidente: 5-33, Candidato Vicepresidente: LUISA GONZALEZ\n",
      "oracion_entrevista: Es la √∫nica manera en la cual t√∫ puedes depurar., Partido: PARTIDO SOCIEDAD PATRI√ìTICA 21 DE ENERO, Candidato Presidente: ANDREA GONZALEZ, Candidato Vicepresidente: GALO MONCAYO\n",
      "oracion_plan: comprometer promover cultura paz nivel sociedad fomentar educaci√≥n basado respeto empat√≠a, Temas clave: cultura, educaci√≥n, Partido: REVOLUCI√ìN CIUDADANA - RETO, Candidato Presidente: 5-33, Candidato Vicepresidente: LUISA GONZALEZ\n",
      "\n",
      "Genera un resumen basado en estos resultados.\n",
      "\n",
      "Respuesta de Ollama:\n",
      "**Resumen de los Resultados**\n",
      "\n",
      "Los resultados presentados se refieren a una serie de discursos pol√≠ticos realizados por candidatos presidenciales y vicepresidentes de diferentes partidos pol√≠ticos en Ecuador. A continuaci√≥n, se presentan los puntos clave y las tendencias identificadas en estos discursos:\n",
      "\n",
      "**Econom√≠a**\n",
      "\n",
      "* El Partido Movimiento de Unidad Plurinacional Pachakutik propone secuestrar a un oligarca banquero para abordar la econom√≠a.\n",
      "* El Partido Socialista Ecuatoriano sugiere \"cometer\" a cobrar impuestos a grandes deudores evasores.\n",
      "* El Partido Unitario Popular busca afectar la econom√≠a familiar ecuatoriana.\n",
      "\n",
      "**Ecosistema y Cultura**\n",
      "\n",
      "* El Movimiento Creo propone restaurar el ecosistema vital del manglar.\n",
      "* La Revoluci√≥n Ciudadana - Reto busca educar y concientizar a la sociedad sobre temas como cultura, educaci√≥n y respeto.\n",
      "\n",
      "**Seguridad y Defensa**\n",
      "\n",
      "* El Partido Social Cristiano sugiere que alguien \"lidiar√°\" con una situaci√≥n de crisis.\n",
      "* El Partido Patri√≥tico 21 de Enero propone fortalecer y garantizar la seguridad de la sociedad.\n",
      "\n",
      "**Educaci√≥n y Cultura**\n",
      "\n",
      "* La Revoluci√≥n Ciudadana - Reto busca educar y concientizar a la sociedad sobre temas como cultura, educaci√≥n y respeto.\n",
      "* El Movimiento Creo promueve la cultura y la paz en la sociedad.\n",
      "\n",
      "**Tendencias Identificadas**\n",
      "\n",
      "* La mayor√≠a de los discursos se centran en temas econ√≥micos y sociales.\n",
      "* Hay una fuerte √©nfasis en la importancia de la educaci√≥n y la conciencia social.\n",
      "* Los partidos pol√≠ticos presentan soluciones creativas y poco convencionales para abordar los problemas nacionales.\n",
      "\n",
      "Es importante destacar que estos discursos pueden ser fragmentados y no reflejan necesariamente las posiciones oficiales de los partidos pol√≠ticos. Sin embargo, ofrecen una visi√≥n interesante sobre las tendencias y prioridades de cada partido en este momento pol√≠tico.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import faiss\n",
    "\n",
    "# Para preprocesamiento\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import spacy\n",
    "\n",
    "# Descargar recursos de NLTK (si a√∫n no se han descargado)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Cargar el modelo spaCy para espa√±ol (aseg√∫rate de tener instalado \"es_core_news_sm\")\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Cargar el tokenizador y el modelo BERT (para obtener embeddings)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def obtener_embedding(texto):\n",
    "    \"\"\"\n",
    "    Dada una cadena de texto, devuelve el embedding correspondiente usando BERT.\n",
    "    Se utiliza el embedding del token [CLS] como representaci√≥n.\n",
    "    \"\"\"\n",
    "    texto = str(texto)  # asegurar que sea string\n",
    "    inputs = tokenizer(texto, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Se toma el embedding del token [CLS] (posici√≥n 0) y se aplana a un vector 1D (768,)\n",
    "    embedding = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "    return embedding\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 1. Cargar los DataFrames con embeddings desde los archivos .pkl\n",
    "# ---------------------------------------------------------------------------\n",
    "with open('oraciones_con_embeddings_completo.pkl', 'rb') as f:\n",
    "    df1 = pickle.load(f)\n",
    "    \n",
    "with open('./data/interview/elecciones_oraciones_con_embeddings.pkl', 'rb') as f:\n",
    "    df2 = pickle.load(f)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 2. Reconstruir los √≠ndices FAISS a partir de los embeddings almacenados en los DataFrames\n",
    "# ---------------------------------------------------------------------------\n",
    "def construir_index_faiss(df, embedding_col):\n",
    "    \"\"\"\n",
    "    Dado un DataFrame y el nombre de la columna que contiene los embeddings,\n",
    "    crea y devuelve un √≠ndice FAISS usando IndexFlatL2.\n",
    "    \"\"\"\n",
    "    embeddings_matrix = np.vstack(df[embedding_col].values).astype('float32')\n",
    "    dim = embeddings_matrix.shape[1]\n",
    "    index = faiss.IndexFlatL2(dim)\n",
    "    index.add(embeddings_matrix)\n",
    "    print(f\"Se agregaron {index.ntotal} vectores al √≠ndice FAISS.\")\n",
    "    return index\n",
    "\n",
    "# Se asume que en df1 la columna de embeddings se llama 'embedding_Oracion'\n",
    "index1 = construir_index_faiss(df1, 'embedding_Oracion')\n",
    "\n",
    "# Se asume que en df2 la columna de embeddings se llama 'embedding_oracion_sinStopWords'\n",
    "index2 = construir_index_faiss(df2, 'embedding_oracion_sinStopWords')\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 3. Funci√≥n para preprocesar la consulta\n",
    "# ---------------------------------------------------------------------------\n",
    "def preprocess_query(query):\n",
    "    \"\"\"\n",
    "    Preprocesa la consulta:\n",
    "      - Pasa a min√∫sculas.\n",
    "      - Tokeniza el texto en espa√±ol.\n",
    "      - Elimina tokens que no sean alfab√©ticos y las stopwords en espa√±ol.\n",
    "      - Lematiza el texto usando spaCy.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(query.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    cleaned_query = ' '.join(lemmatized_tokens)\n",
    "    print(f\"Consulta preprocesada: {cleaned_query}\")\n",
    "    return cleaned_query\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# 4. Funci√≥n para hacer la consulta en ambos √≠ndices y usar Ollama para generar una respuesta\n",
    "# ---------------------------------------------------------------------------\n",
    "def query_faiss_ollama(query_text, k=5, n_top=5):\n",
    "    \"\"\"\n",
    "    Preprocesa la consulta, obtiene su embedding y la consulta en dos √≠ndices FAISS.\n",
    "    Combina los resultados, ordena por similitud (menor distancia) y toma solo los n_top m√°s similares.\n",
    "    Luego construye un prompt para enviar a Ollama.\n",
    "    \n",
    "    Par√°metros:\n",
    "      - query_text: texto de consulta.\n",
    "      - k: n√∫mero de vecinos a recuperar en cada √≠ndice.\n",
    "      - n_top: n√∫mero final de resultados (combinados de ambos √≠ndices) a utilizar.\n",
    "    \"\"\"\n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_query(query_text)\n",
    "    \n",
    "    # Generar el embedding para la consulta (aseg√∫rate de que sea float32)\n",
    "    query_embedding = obtener_embedding(cleaned_query).astype('float32')\n",
    "    query_vector = np.expand_dims(query_embedding, axis=0)\n",
    "    \n",
    "    # Buscar en el primer √≠ndice (df1)\n",
    "    distances1, indices1 = index1.search(query_vector, k)\n",
    "    # Buscar en el segundo √≠ndice (df2)\n",
    "    distances2, indices2 = index2.search(query_vector, k)\n",
    "    \n",
    "    # Combinar resultados de ambos √≠ndices junto con sus distancias\n",
    "    resultados_con_dist = []\n",
    "    \n",
    "    # Procesar resultados del primer √≠ndice (df1)\n",
    "    for idx, dist in zip(indices1[0], distances1[0]):\n",
    "        fila = df1.iloc[idx]\n",
    "        # Se asume que df1 tiene una columna \"Oracion\" para la oraci√≥n original\n",
    "        resultado = (\n",
    "            f\"oracion_plan: {fila['Oracion']}, \"\n",
    "            f\"Temas clave: {fila['Temas Clave']}, \"\n",
    "            f\"Partido: {fila['Partido']}, \"\n",
    "            f\"Candidato Presidente: {fila['ListaPolitica']}, \"\n",
    "            f\"Candidato Vicepresidente: {fila['CandidatoPresidente']}\"\n",
    "        )\n",
    "        resultados_con_dist.append((dist, resultado))\n",
    "    \n",
    "    # Procesar resultados del segundo √≠ndice (df2)\n",
    "    for idx, dist in zip(indices2[0], distances2[0]):\n",
    "        fila = df2.iloc[idx]\n",
    "        # Se asume que df2 tiene una columna \"oracion_original\" para la oraci√≥n original\n",
    "        resultado = (\n",
    "            f\"oracion_entrevista: {fila['oracion_original']}, \"\n",
    "            f\"Partido: {fila['Partido']}, \"\n",
    "            f\"Candidato Presidente: {fila['presidente']}, \"\n",
    "            f\"Candidato Vicepresidente: {fila['vicepresidente']}\"\n",
    "        )\n",
    "        resultados_con_dist.append((dist, resultado))\n",
    "    \n",
    "    # Ordenar los resultados combinados por distancia (la menor distancia indica mayor similitud)\n",
    "    resultados_con_dist.sort(key=lambda x: x[0])\n",
    "    \n",
    "    # Tomar solo los n_top resultados con mayor similitud\n",
    "    resultados_finales = [resultado for _, resultado in resultados_con_dist[:n_top]]\n",
    "    \n",
    "    # Construir el prompt para enviar a Ollama\n",
    "    prompt = (\n",
    "        \"Esto es lo que he encontrado.:\\n\\n\" +\n",
    "        \"\\n\".join(resultados_finales) +\n",
    "        \"\\n\\nGenera un resumen basado en estos resultados.\"\n",
    "    )\n",
    "    print(\"\\nPrompt para Ollama:\")\n",
    "    print(prompt)\n",
    "    \n",
    "    # Llamar a Ollama (aseg√∫rate de tener la librer√≠a 'ollama' instalada y configurada)\n",
    "    import ollama\n",
    "    respuesta = ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "    return respuesta['message']['content']\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Ejemplo de consulta\n",
    "# ---------------------------------------------------------------------------\n",
    "query_text = \"bajar el iva para fortalecer la economia\"\n",
    "print(\"\\n--- Realizando consulta en ambos √≠ndices ---\")\n",
    "# En este ejemplo, se buscan 50 vecinos en cada √≠ndice, pero se usar√°n solo los 5 resultados\n",
    "respuesta_ollama = query_faiss_ollama(query_text, k=50, n_top=50)\n",
    "print(\"\\nRespuesta de Ollama:\")\n",
    "print(respuesta_ollama)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto preprocesado: que propone daniel noboa para el fortalecimiento economico?\n",
      "Consulta procesada: proponer daniel noboa fortalecimiento economico\n",
      "Bas√°ndome en las declaraciones proporcionadas, se puede resumir que el tema principal de estas discursos y manifestaciones pol√≠ticas es la promoci√≥n de pol√≠ticas y acciones que benefician al pa√≠s Ecuador, priorizando la soberan√≠a nacional, la defensa del medio ambiente, la sostenibilidad econ√≥mica y social, la seguridad ciudadana y la protecci√≥n de los derechos humanos.\n",
      "\n",
      "Muchos de los partidos pol√≠ticos mencionados en estas declaraciones, como Movimiento Creo, Movimiento Construye, Partido Sociedad Patri√≥tica 21 de Enero, Partido Sociedad Unida M√°s Acci√≥n, Movimiento Amigo y Movimiento Centro Democr√°tico, enfatizan la importancia de:\n",
      "\n",
      "1. **Desarrollo sostenible**: Promover pol√≠ticas que equilibren el crecimiento econ√≥mico con la protecci√≥n ambiental y la conservaci√≥n de los recursos naturales.\n",
      "2. **Soberan√≠a nacional**: Priorizar la independencia y autonom√≠a del pa√≠s en sus relaciones internacionales, especialmente en la integraci√≥n regional y en la cooperaci√≥n internacional.\n",
      "3. **Seguridad ciudadana**: Asegurar la seguridad y la estabilidad social, protegiendo los derechos humanos y promoviendo la justicia social.\n",
      "4. **Econom√≠a equitativa**: Propiciar pol√≠ticas econ√≥micas que favorezcan a la poblaci√≥n en general, reduciendo la brecha entre riqueza y pobreza.\n",
      "5. **Desarrollo urbano y vial**: Mejorar el estado del transporte p√∫blico y la infraestructura urbana para facilitar el acceso a servicios b√°sicos y promover un crecimiento econ√≥mico sostenible.\n",
      "\n",
      "En general, estos discursos reflejan una visi√≥n de desarrollo social y econ√≥mico equilibrado, que priorice el bienestar de los ciudadanos y la protecci√≥n del medio ambiente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "import ollama  # Importar la librer√≠a para usar Ollama\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV con las oraciones procesadas\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "\n",
    "# Cargar el archivo CSV con los datos de los partidos y candidatos\n",
    "\n",
    "candidatos_df = pd.read_csv('candidatos.csv', delimiter=';')  # Aseg√∫rate de que el archivo correcto est√© aqu√≠\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si est√° disponible\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cargar el √≠ndice FAISS previamente guardado\n",
    "index = faiss.read_index('faiss_index.index')\n",
    "\n",
    "# Cargar los datos de oraciones\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el modelo de spaCy para lematizaci√≥n en espa√±ol\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Funci√≥n para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    print(f\"Texto preprocesado: {text}\")\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Funci√≥n para obtener la oraci√≥n original sin procesar y sus datos adicionales\n",
    "def get_oracion_data_by_id(oracion_id):\n",
    "    oracion_row = df[df['Oracion_ID'] == oracion_id]\n",
    "    if not oracion_row.empty:\n",
    "        oracion = oracion_row.iloc[0]['Oracion']\n",
    "        temas_clave = oracion_row.iloc[0]['Temas Clave']\n",
    "        return oracion, temas_clave\n",
    "    return None, None\n",
    "\n",
    "def get_partido_data_by_id(id):\n",
    "    candidato_row = candidatos_df[candidatos_df['ID'] == id]  # Buscamos usando el 'ID'\n",
    "    if not candidato_row.empty:\n",
    "        partido = candidato_row.iloc[0]['Partido']\n",
    "        candidato_presidente = candidato_row.iloc[0]['CandidatoPresidente']\n",
    "        candidato_vicepresidente = candidato_row.iloc[0]['CandidatoVicePresidente']\n",
    "        lista_politica = candidato_row.iloc[0]['ListaPolitica']\n",
    "        return partido, candidato_presidente, candidato_vicepresidente, lista_politica\n",
    "    return None, None, None, None\n",
    "\n",
    "def query_faiss_ollama(query, top_k=50):  \n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    # Mostrar el tama√±o de la consulta procesada\n",
    "    print(f\"Consulta procesada: {cleaned_query}\")\n",
    "    # Generar el embedding para la consulta\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    # Realizar la b√∫squeda en el √≠ndice FAISS\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    # Procesar los resultados\n",
    "    resultados = []\n",
    "    for i in range(top_k):\n",
    "        # Obtener el ID de la oraci√≥n correspondiente\n",
    "        oracion_id = sentences[I[0][i]]['Oracion_ID']\n",
    "\n",
    "        id= sentences[I[0][i]]['ID']\n",
    "        # Buscar la oraci√≥n original y sus temas clave usando Oracion_ID\n",
    "        oracion_original, temas_clave = get_oracion_data_by_id(oracion_id)\n",
    "\n",
    "        # Buscar los datos del partido usando el ID del candidato\n",
    "        partido, candidato_presidente, candidato_vicepresidente, lista_politica = get_partido_data_by_id(id)\n",
    "\n",
    "        if oracion_original:  # Si se encuentra la oraci√≥n original\n",
    "            resultado = (\n",
    "                f\"Oraci√≥n: {oracion_original}, \"\n",
    "                f\"Temas Clave: {temas_clave}, \"\n",
    "                f\"Partido: {partido}, \"\n",
    "                f\"Presidente: {candidato_presidente}, \"\n",
    "                f\"Vicepresidente: {candidato_vicepresidente}, \"\n",
    "                f\"Lista Pol√≠tica: {lista_politica}\"\n",
    "            )\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "    # Verificar si hemos obtenido resultados\n",
    "    if not resultados:\n",
    "        return \"No se encontraron oraciones relevantes para la consulta.\"\n",
    "\n",
    "    # Construir el prompt para Ollama con las oraciones obtenidas\n",
    "    prompt = f\"quiero que menciones los nombres de los Presidente,Vicepresidente,Lista Pol√≠tica y el partido de lo que he encontrado clasificalo:\\n\\n\" + \"\\n\".join(resultados) + \"\\n\\nGenera un resumen basado en estas declaraciones, expl√≠calo.\"\n",
    "\n",
    "    # Generar la respuesta con Ollama\n",
    "    respuesta = ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "    return respuesta['message']['content']\n",
    "\n",
    "# Ejemplo de consulta para verificar el funcionamiento\n",
    "query = \"que propone daniel noboa para el fortalecimiento economico?\"\n",
    "respuesta = query_faiss_ollama(query)\n",
    "print(respuesta)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
