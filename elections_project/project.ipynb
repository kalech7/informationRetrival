{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Función para extraer texto del PDF excluyendo las primeras y últimas páginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extraído\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuración global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a imágenes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID único a cada oración\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID único para cada oración\n",
    "            oraciones_texto.append(oracion)  # Texto de la oración\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA SÍ\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo oraciones.csv procesado, limpiado y ordenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeración de página (ejemplo: 'Página 1', 'pág. 2', etc.)\n",
    "    text = re.sub(r'Página \\d+', '', text)\n",
    "    text = re.sub(r'pág\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfabéticos ni numéricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", ordenar por ID y guardar\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  \n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Ordenar las filas por ID (conversión a int para evitar errores de ordenación)\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "\n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado, limpiado y ordenado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtokenize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sent_tokenize\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Cargar modelo de lenguaje en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el CSV con las oraciones\n",
    "df = pd.read_csv(\"oraciones.csv\", sep=\";\")\n",
    "\n",
    "# Lista ampliada de temas clave\n",
    "temas_relevantes = {\n",
    "    \"economía\", \"educación\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupción\", \"tecnología\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"política\", \"desarrollo\", \"energía\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovación\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversión\",\n",
    "    \"vivienda\", \"servicios públicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educación superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalización\", \"gestión pública\", \"comercio\",\n",
    "    \"cambio climático\"\n",
    "}\n",
    "\n",
    "# Función para extraer propuestas y detectar temas clave\n",
    "def extraer_propuestas_y_temas(texto):\n",
    "    if pd.isna(texto):  # Manejar valores nulos\n",
    "        return \"\", \"\"\n",
    "\n",
    "    oraciones = sent_tokenize(texto, language=\"spanish\")  # Dividir en oraciones\n",
    "    propuestas = []\n",
    "    temas_detectados = set()\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        doc = nlp(oracion)  \n",
    "        \n",
    "        # Detectar oraciones con verbos de acción\n",
    "        if any(token.pos_ == \"VERB\" for token in doc):\n",
    "            propuestas.append(oracion)\n",
    "\n",
    "            # Identificar temas clave dentro de la oración\n",
    "            temas_detectados.update({tema for tema in temas_relevantes if tema in oracion.lower()})\n",
    "\n",
    "    # Tomar hasta 3 propuestas clave\n",
    "    return \"; \".join(propuestas[:3]), \", \".join(temas_detectados)\n",
    "\n",
    "# Aplicar la extracción en cada fila del DataFrame\n",
    "df[[\"Propuestas\", \"Temas Clave\"]] = df[\"Oracion\"].apply(lambda txt: pd.Series(extraer_propuestas_y_temas(txt)))\n",
    "\n",
    "# Guardar el nuevo CSV con las nuevas columnas sin eliminar datos anteriores\n",
    "df.to_csv(\"oraciones_actualizado.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\"✅ CSV actualizado con éxito. Se agregaron las columnas 'Propuestas' y 'Temas Clave'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alech/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer  # Usamos stemmer para español\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Función para tokenizar, eliminar stopwords y aplicar stemming\n",
    "def preprocesar_texto(texto):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(texto.lower(), language='spanish')\n",
    "    \n",
    "    # Eliminar stopwords y caracteres no alfabéticos\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [word for word in tokens \n",
    "              if word.isalpha() \n",
    "              and word not in stop_words\n",
    "              and len(word) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", eliminar filas vacías y luego escribir las filas nuevamente\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: tokenizar, eliminar stopwords y aplicar stemming\n",
    "                        row['Oracion'] = preprocesar_texto(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  # Si no está vacío o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "input_csv = 'oraciones.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert y faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301a614fa1794f4aa240f733d1312ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('oraciones_procesadas.csv', delimiter=';')  # Cambia el nombre de tu archivo CSV si es necesario\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si está disponible\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "\n",
    "# Convertir los embeddings en un formato compatible con FAISS (float32)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un índice FAISS\n",
    "dimension = embeddings.shape[1]  # Dimensión de los embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # Índice basado en L2 (distancia euclidiana)\n",
    "index.add(embeddings)  # Agregar los embeddings al índice FAISS\n",
    "\n",
    "# Guardar los embeddings y el índice FAISS en archivos\n",
    "np.save('embeddings.npy', embeddings)  # Guardamos los embeddings en un archivo .npy\n",
    "faiss.write_index(index, 'faiss_index.index')  # Guardamos el índice FAISS en un archivo .index\n",
    "\n",
    "# Guardar los ID y las oraciones en un archivo .pkl para cargarlos fácilmente después\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df[['ID', 'Oracion']].to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID                                            Oracion  \\\n",
      "0     1  recuper gratuid prepar ingres escuel polic nac...   \n",
      "1     1  promov prevencion violenci muj edad tempran tr...   \n",
      "2     1  establec mecan inmigr edad escol menor años as...   \n",
      "3     1  foment educ artist particip joven activ cultur...   \n",
      "4     2  construccion hospital escuel carreter provinci...   \n",
      "..   ..                                                ...   \n",
      "109  11  cumpl plan nacional reinsercion estudiantil mi...   \n",
      "110  11  contrat especial salud mental dentr escuel col...   \n",
      "111  11  promov profesionaliz artist popular vincul doc...   \n",
      "112  11  mism polit soberan concesion red fortalec empr...   \n",
      "113  15  signif sol cubr demand dej estudi pued acced s...   \n",
      "\n",
      "                                      Partido  \n",
      "0                REVOLUCIÓN CIUDADANA - RETO   \n",
      "1                REVOLUCIÓN CIUDADANA - RETO   \n",
      "2                REVOLUCIÓN CIUDADANA - RETO   \n",
      "3                REVOLUCIÓN CIUDADANA - RETO   \n",
      "4    PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA   \n",
      "..                                        ...  \n",
      "109                   PARTIDO UNIDAD POPULAR   \n",
      "110                   PARTIDO UNIDAD POPULAR   \n",
      "111                   PARTIDO UNIDAD POPULAR   \n",
      "112                   PARTIDO UNIDAD POPULAR   \n",
      "113                 PARTIDO SOCIAL CRISTIANO   \n",
      "\n",
      "[114 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para aplicar stemming a un texto\n",
    "def stem_text(text):\n",
    "    words = nltk.word_tokenize(text, language=\"spanish\")  # Tokenizar el texto\n",
    "    words_stemmed = [stemmer.stem(word) for word in words]  # Aplicar stemming\n",
    "    return \" \".join(words_stemmed)  # Reunir las palabras nuevamente en una oración\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_stemmed = stem_text(query)  # Aplicar stemming a la consulta\n",
    "\n",
    "    # Filtrar las oraciones que contengan términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_stemmed in stem_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, buscamos los partidos\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        print(df_resultados[['ID', 'Oracion', 'Partido']])\n",
    "    else:\n",
    "        print(\"No se encontraron oraciones que coincidan con la consulta.\")\n",
    "\n",
    "# Función para buscar el partido según el ID en `candidatos.csv`\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Ejemplo de consulta:\n",
    "query = \"escuelas\"\n",
    "buscar_por_consulta(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so I'm trying to help someone with their query about the economy and political parties. They provided a list of IDs, sentences, and corresponding parties. The task is to summarize this information in a clear and understandable way.\n",
      "\n",
      "First, I'll need to read through each entry carefully. It looks like each ID has multiple oraciones (sentences) related to economic topics, each tied to a specific partido político (political party). My goal is to group these sentences by their respective parties and then explain what each party is focusing on economically.\n",
      "\n",
      "Starting with ID 2, all the sentences are from \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA.\" The topics here seem to be about economic development, maybe creating special zones or boards for economic growth, similar to Singapore's model. So I'll note that this party is talking about economic development and special zones.\n",
      "\n",
      "Next, ID 4 is from \"MOVIMIENTO CENTRO DEMOCRÁTICO.\" The sentences mention things like economic-social inclusion and policies aimed at reducing inequality and poverty. It seems like they're focusing on social aspects of the economy, so I'll highlight their focus on social inclusion and reducing disparities.\n",
      "\n",
      "ID 5 corresponds to \"MOVIMIENTO CONSTRUYE.\" Their sentences discuss foreign investment in sectors like oil production. They seem concerned with economic stability through investments and job creation, especially in industries that are key to the national economy. So I'll note their emphasis on foreign investment and economic recovery through specific programs.\n",
      "\n",
      "ID 6 is linked to \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES.\" The sentences talk about rural economic opportunities and employment growth. They seem focused on fostering economic growth in rural areas, possibly through job creation and sustainable practices. I'll mention their emphasis on rural economy and job opportunities.\n",
      "\n",
      "ID 10 is from \"PARTIDO SOCIEDAD PATRIÓTICA 21 DE ENERO.\" Their sentences discuss tourism promotion and security to attract foreign tourists, referencing a report from the World Economic Forum. It looks like they're addressing regional economic stability through tourism and improving perceptions of safety. I'll note their focus on tourism and regional stability.\n",
      "\n",
      "ID 16 is tied to \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK.\" The sentences here talk about public funds aimed at supporting small family economies, especially in agriculture, and fostering food sovereignty. They seem focused on sustainable economic practices and helping small farmers adapt to climate change. I'll highlight their support for small-scale farming and food sovereignty.\n",
      "\n",
      "ID 11 is from \"PARTIDO UNIDAD POPULAR.\" Their sentences mention a multi-year government plan aimed at recovering the economy by generating specific jobs and objectives, suggesting they're planning long-term economic recovery strategies. I'll note their focus on economic recovery through strategic plans.\n",
      "\n",
      "Lastly, ID 15 corresponds to \"PARTIDO SOCIAL CRISTIANO.\" The sentences discuss implementing policies and programs to promote formalization in the labor market, particularly in specialized areas. It seems like they're pushing for more structured and regulated labor practices. I'll mention their emphasis on formalizing the economy through specific policies.\n",
      "\n",
      "Putting it all together, each political party has distinct economic focuses ranging from development strategies and social inclusion to rural growth, tourism, small-scale agriculture, long-term recovery, and labor market formalization. My summary should clearly outline these points in a natural, easy-to-understand manner.\n",
      "</think>\n",
      "\n",
      "Basado en la información proporcionada, se puede observar cómo diferentes partidos políticos abordan temas relacionados con la economía en Ecuador:\n",
      "\n",
      "1. **PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN (SUMA)**:\n",
      "   - Se enfoca en el desarrollo económico y la creación de zonas económicas especiales, similar al modelo de Singapur.\n",
      "   - Ejemplos incluyen la implementación de un \"Economic Development Board\" para promover inversiones y fomentar el desarrollo económico.\n",
      "\n",
      "2. **MOVIMIENTO CENTRO DEMOCRÁTICO**:\n",
      "   - Aborda temas de inclusión social y reducción de desigualdades a través de políticas económicas y sociales.\n",
      "   - Busca implementar modelos de desarrollo que garanticen la equidad y el acceso a recursos económicos.\n",
      "\n",
      "3. **MOVIMIENTO CONSTRUYE**:\n",
      "   - Discute sobre la importancia de las inversiones extranjeras directas y su impacto en la economía nacional, especialmente en sectores clave como la producción petrolera.\n",
      "   - Se menciona la necesidad de promover el empleo y garantizar el crecimiento económico a través de planes claros.\n",
      "\n",
      "4. **MOVIMIENTO CREO (CREANDO OPORTUNIDADES)**:\n",
      "   - Se centra en fomentar el desarrollo económico rural y la generación de empleos.\n",
      "   - Busca impulsar el crecimiento económico mediante la creación de oportunidades laborales y la explotación de recursos naturales.\n",
      "\n",
      "5. **PARTIDO SOCIEDAD PATRIÓTICA (21 DE ENERO)**:\n",
      "   - Aborda temas de seguridad turística y la necesidad de promover la economía regional.\n",
      "   - Se menciona la importancia de mejorar la percepción de seguridad para atraer más turistas extranjeros.\n",
      "\n",
      "6. **MOVIMIENTO DE UNIDAD PLURINACIONAL (PACHAKUTIK)**:\n",
      "   - Fomenta el apoyo a las economías familiares campesinas y la soberanía alimentaria.\n",
      "   - Propone subsidios directos para garantizar la seguridad alimentaria y adaptar a los cambios climáticos.\n",
      "\n",
      "7. **PARTIDO UNIDAD POPULAR**:\n",
      "   - Discute sobre planes plurianuales para recuperar la economía, con énfasis en generar empleos dignos y alcanzar objetivos específicos.\n",
      "   - Se enfoca en políticas económicas que promuevan el crecimiento sostenible.\n",
      "\n",
      "8. **PARTIDO SOCIAL CRISTIANO**:\n",
      "   - Busca implementar políticas y programas para formalizar la economía, especialmente en el mercado laboral.\n",
      "   - Promueve la especialización en áreas específicas del mercado laboral para mejorar la productividad y la estabilidad económica.\n",
      "\n",
      "Cada partido político aborda temas económicos de manera distinta, adaptando sus propuestas a sus visiones y objetivos políticos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import ollama\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para aplicar stemming a un texto\n",
    "def stem_text(text):\n",
    "    words = nltk.word_tokenize(text, language=\"spanish\")  # Tokenizar el texto\n",
    "    words_stemmed = [stemmer.stem(word) for word in words]  # Aplicar stemming\n",
    "    return \" \".join(words_stemmed)  # Reunir las palabras nuevamente en una oración\n",
    "\n",
    "# Función para buscar el partido según el ID en candidatos.csv\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_stemmed = stem_text(query)  # Aplicar stemming a la consulta\n",
    "\n",
    "    # Filtrar las oraciones que contengan términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_stemmed in stem_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, agregamos el partido\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        return df_resultados[['ID', 'Oracion', 'Partido']]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función para generar respuesta en lenguaje natural con Ollama\n",
    "def generar_respuesta(query):\n",
    "    resultados = buscar_por_consulta(query)\n",
    "    \n",
    "    if resultados is not None:\n",
    "        # Convertimos los resultados en un texto para Ollama\n",
    "        texto_resultados = \"\\n\".join([f\"ID: {row['ID']}, Oración: {row['Oracion']}, Partido: {row['Partido']}\" \n",
    "                                      for _, row in resultados.iterrows()])\n",
    "\n",
    "        # Generamos la respuesta con Ollama\n",
    "        prompt = f\"\"\"\n",
    "        Basado en la consulta '{query}', encontré estas oraciones y sus respectivos partidos:\n",
    "        \n",
    "        {texto_resultados}\n",
    "\n",
    "        Responde en lenguaje natural resumiendo la información de manera clara y comprensible.\n",
    "        \"\"\"\n",
    "        respuesta = ollama.chat(model='deepseek-r1:14b', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "        return respuesta['message']['content']\n",
    "    else:\n",
    "        return \"No se encontraron resultados para tu consulta.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"economia\"\n",
    "respuesta_final = generar_respuesta(query)\n",
    "print(respuesta_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alech/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Bueno, el usuario me ha proporcionado una información relevante que extrajo de la pregunta \"¿Cómo afecta la economía al país?\". La información incluye un ID (2), una oración en español, y menciona a un partido político llamado \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA\".\n",
      "\n",
      "Primero, entiendo que el usuario quiere una lista completa de candidatos con esa misma información. Sin embargo, solo ha proporcionado un ejemplo con ID 2 y ORACIÓN, así como el nombre del partido.\n",
      "\n",
      "Para generar una respuesta adecuada, necesito asumir que hay más candidatos y posiblemente más IDs y oraciones relacionadas con la economía y su impacto en el país. Imagino que cada candidato podría tener un ID único, una oración relevante y pertenecer a un partido político.\n",
      "\n",
      "Después de realizar esta asunción, procedo a crear una lista de candidatos imaginaria, asignando IDs consecutivos (desde 1 en adelante), creando oraciones relevantes que conectan la economía con el país, y atribuyendo cada uno al partido \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA\" como se mencionó.\n",
      "\n",
      "Es importante asegurarme de que las oraciones sean variadas y reflejen diferentes aspectos de cómo la economía puede afectar al país. Además, estructuro cada candidato en un formato consistente para mayor claridad.\n",
      "\n",
      "Finalmente, agrego una nota al final de la respuesta pidiendo más detalles si el usuario tiene una fuente específica o más información que pueda compartir, lo cual ayudaría a proporcionar una respuesta más precisa y personalizada.\n",
      "</think>\n",
      "\n",
      "Claro, basándome en la información proporcionada, aquí tienes una lista imaginaria de candidatos con los mismos parámetros:\n",
      "\n",
      "1.  \n",
      "   - **ID:** 1  \n",
      "   - **Oración:** \"El crecimiento económico sostenible es clave para el desarrollo del país y mejorar la calidad de vida de los ciudadanos.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "2.  \n",
      "   - **ID:** 3  \n",
      "   - **Oración:** \"Necesitamos implementar políticas económicas que incentiven el emprendimiento y reduzcan la desigualdad social.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "3.  \n",
      "   - **ID:** 4  \n",
      "   - **Oración:** \"La estabilidad financiera es fundamental para atraer inversiones y fortalecer la economía del país.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "4.  \n",
      "   - **ID:** 5  \n",
      "   - **Oración:** \"El sector fintech tiene un gran potencial para impulsar el crecimiento económico y modernizar el sistema financiero ecuatoriano.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "Si tienes más información o una fuente específica, podría ayudarte a proporcionar una respuesta más precisa.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import ollama\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el stemmer en español y cargar stopwords\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para preprocesar el texto: tokenización, eliminación de stopwords y stemming\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text, language=\"spanish\")  # Tokenizar el texto\n",
    "    words_filtered = [word for word in words if word.isalnum() and word.lower() not in stop_words]  # Eliminar stopwords\n",
    "    words_stemmed = [stemmer.stem(word) for word in words_filtered]  # Aplicar stemming\n",
    "    return \" \".join(words_stemmed)  # Reunir las palabras nuevamente en una oración\n",
    "\n",
    "# Función para obtener el partido según el ID en candidatos.csv\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_processed = preprocess_text(query)  # Preprocesar la consulta del usuario\n",
    "\n",
    "    # Filtrar oraciones con términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_processed in preprocess_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, agregar la columna de partidos\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        return df_resultados[['ID', 'Oracion', 'Partido']]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función para generar respuesta en lenguaje natural con Ollama\n",
    "def generar_respuesta(query):\n",
    "    resultados = buscar_por_consulta(query)\n",
    "    \n",
    "    if resultados is not None:\n",
    "        # Convertimos los resultados en un texto para Ollama\n",
    "        texto_resultados = \"\\n\".join([f\"ID: {row['ID']}, Oración: {row['Oracion']}, Partido: {row['Partido']}\" \n",
    "                                      for _, row in resultados.iterrows()])\n",
    "\n",
    "        # Generamos la respuesta con Ollama\n",
    "        prompt = f\"\"\"\n",
    "        Basado en '{query}', encontré la siguiente información relevante:\n",
    "\n",
    "        {texto_resultados}\n",
    "\n",
    "        dame la lista entera de canditados con esa informacion\n",
    "        \"\"\"\n",
    "        respuesta = ollama.chat(model='deepseek-r1:14b', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "        return respuesta['message']['content']\n",
    "    else:\n",
    "        return \"No se encontraron resultados para tu consulta.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¿Cómo afecta la economía al país?\"\n",
    "respuesta_final = generar_respuesta(query)\n",
    "print(respuesta_final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
