{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# entrevistas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Función para extraer texto del PDF excluyendo las primeras y últimas páginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extraído\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuración global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a imágenes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID único a cada oración\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID único para cada oración\n",
    "            oraciones_texto.append(oracion)  # Texto de la oración\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA SÍ\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo oraciones.csv procesado, limpiado y ordenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeración de página (ejemplo: 'Página 1', 'pág. 2', etc.)\n",
    "    text = re.sub(r'Página \\d+', '', text)\n",
    "    text = re.sub(r'pág\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfabéticos ni numéricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", ordenar por ID y guardar\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  \n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Ordenar las filas por ID (conversión a int para evitar errores de ordenación)\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "\n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado, limpiado y ordenado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CSV actualizado con éxito. Se agregaron las columnas 'Temas Clave'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Cargar modelo de lenguaje en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el CSV con las oraciones\n",
    "df = pd.read_csv(\"oraciones.csv\", sep=\";\")\n",
    "\n",
    "# Lista ampliada de temas clave\n",
    "temas_relevantes = {\n",
    "    \"economía\", \"educación\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupción\", \"tecnología\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"política\", \"desarrollo\", \"energía\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovación\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversión\",\n",
    "    \"vivienda\", \"servicios públicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educación superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalización\", \"gestión pública\", \"comercio\",\n",
    "    \"cambio climático\", \"energías renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud pública\", \"gobernanza\", \"justicia social\", \"igualdad de género\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestación\", \"movilidad urbana\", \"biodiversidad\", \"educación financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnológica\",\n",
    "    \"educación digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnología\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"economía circular\",\n",
    "    \"ciudades inteligentes\", \"protección de datos\", \"energía solar\", \"transporte eléctrico\",\n",
    "    \"robotización\", \"computación cuántica\", \"espacio exterior\", \"protección ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad eléctrica\", \"alimentos orgánicos\", \"tecnología educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas autónomos\",\n",
    "    \"tecnología espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energía eólica\", \"tecnologías disruptivas\", \"energía geotérmica\",\n",
    "    \"nanotecnología\", \"microbioma\", \"bioeconomía\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energía limpia\", \"criptomonedas\", \"minería digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educación en línea\",\n",
    "    \"biomimicry\", \"ecoinnovación\", \"simulación computacional\", \"agricultura urbana\", \"cultivos inteligentes\"\n",
    "}\n",
    "\n",
    "# Función para extraer solo los temas clave\n",
    "def extraer_temas_clave(texto):\n",
    "    if pd.isna(texto):  # Manejar valores nulos\n",
    "        return \"\"\n",
    "\n",
    "    oraciones = sent_tokenize(texto, language=\"spanish\")  # Dividir en oraciones\n",
    "    temas_detectados = set()\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        # Identificar temas clave dentro de la oración\n",
    "        temas_detectados.update({tema for tema in temas_relevantes if tema in oracion.lower()})\n",
    "\n",
    "    # Retornar los temas clave detectados como una cadena separada por coma\n",
    "    return \", \".join(temas_detectados)\n",
    "\n",
    "# Aplicar la extracción de temas clave en cada fila del DataFrame\n",
    "df[\"Temas Clave\"] = df[\"Oracion\"].apply(extraer_temas_clave)\n",
    "\n",
    "# Guardar el nuevo CSV con la nueva columna 'Temas Clave' sin eliminar datos anteriores\n",
    "df.to_csv(\"oraciones_actualizado.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\" CSV actualizado con éxito. Se agregaron las columnas 'Temas Clave'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,lemmatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stopwords en español de NLTK\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Función para lematizar y eliminar stopwords\n",
    "def lematizar_y_eliminar_stopwords(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Lematizar y eliminar stopwords\n",
    "    return ' '.join([token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words])\n",
    "\n",
    "# Limpiar contenido eliminando puntuaciones y números\n",
    "def clean_content(texto):\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar puntuaciones\n",
    "    texto = re.sub(r'\\d+', '', texto)      # Eliminar números\n",
    "    return texto.lower()\n",
    "\n",
    "# Función para procesar el CSV\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: lematizar y eliminar stopwords\n",
    "                        row['Oracion'] = lematizar_y_eliminar_stopwords(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  # Si no está vacío o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion', 'Temas Clave'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "input_csv = 'oraciones_actualizado.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo combinado guardado como 'oraciones_procesadas_completo.csv'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar los archivos CSV asegurando el delimitador correcto\n",
    "df_oraciones = pd.read_csv('oraciones_procesadas.csv', delimiter=';')\n",
    "df_candidatos = pd.read_csv('candidatos.csv', delimiter=';')\n",
    "\n",
    "# Limpiar la columna 'ID' eliminando espacios y asegurando que solo contenga números\n",
    "df_oraciones['ID'] = df_oraciones['ID'].astype(str).str.extract('(\\d+)').astype(float).astype('Int64')\n",
    "df_candidatos['ID'] = df_candidatos['ID'].astype(str).str.extract('(\\d+)').astype(float).astype('Int64')\n",
    "\n",
    "# Realizar la fusión de datos usando 'ID' como clave\n",
    "df_completo = df_oraciones.merge(df_candidatos, on='ID', how='left')\n",
    "\n",
    "# Guardar el nuevo CSV con los datos combinados\n",
    "df_completo.to_csv('oraciones_procesadas_completo.csv', sep=';', index=False)\n",
    "\n",
    "print(\"Archivo combinado guardado como 'oraciones_procesadas_completo.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df962099739240fa91048bf43a4bbb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n",
      "{'ID': 4, 'Oracion_ID': '993', 'Oracion': 'eficiencia energético implementar política eficiencia energético sector clave industria transporte construcción promover uso tecnología reducir consumo energía', 'Temas Clave': 'energía, transporte, ciencia, tecnología, política, industria', 'Partido': 'MOVIMIENTO CENTRO DEMOCRÁTICO', 'CandidatoPresidente': 'JIMMY JAIRALA VALLAZZA', 'CandidatoVicePresidente': 'LUCIA VALLECILLA SUAREZ', 'ListaPolitica': '1', 'Distancia': 0.33702278}\n",
      "{'ID': 14, 'Oracion_ID': '811', 'Oracion': 'además realizar campaña nacional eficiencia ahorro energético énfasis sector comercial residencial industrial institucional', 'Temas Clave': 'industria, ciencia', 'Partido': 'PARTIDO AVANZA', 'CandidatoPresidente': 'LUIS FELIPE TILLERIA', 'CandidatoVicePresidente': 'KARLA PAULINA ROSERO', 'ListaPolitica': '8', 'Distancia': 0.38165388}\n",
      "{'ID': 14, 'Oracion_ID': '827', 'Oracion': 'campaña nacional eficiencia ahorro energético promover práctica eficiencia ahorro energético sector', 'Temas Clave': 'ciencia', 'Partido': 'PARTIDO AVANZA', 'CandidatoPresidente': 'LUIS FELIPE TILLERIA', 'CandidatoVicePresidente': 'KARLA PAULINA ROSERO', 'ListaPolitica': '8', 'Distancia': 0.4163392}\n",
      "{'ID': 8, 'Oracion_ID': '433', 'Oracion': 'impulso inversión privado gobierno deber crear entorno atractivo inversión privado sector energético', 'Temas Clave': 'gobierno, inversión', 'Partido': 'MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID', 'CandidatoPresidente': 'VICTOR ARAUS', 'CandidatoVicePresidente': 'CRISTINA CARRERA', 'ListaPolitica': '4', 'Distancia': 0.42793632}\n",
      "{'ID': 1, 'Oracion_ID': '533', 'Oracion': 'intervenir vivienda promover eficiencia energético', 'Temas Clave': 'vivienda, ciencia', 'Partido': 'REVOLUCIÓN CIUDADANA - RETO', 'CandidatoPresidente': 'LUISA GONZALEZ', 'CandidatoVicePresidente': 'DIEGO BORJA', 'ListaPolitica': '5-33', 'Distancia': 0.42992368}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar los archivos CSV\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "\n",
    "\n",
    "# Inicializar el modelo BERT y moverlo a GPU si está disponible\n",
    "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un índice FAISS\n",
    "dimension = embeddings.shape[1]\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "index.add(embeddings)\n",
    "\n",
    "# Guardar los embeddings e índice FAISS\n",
    "np.save('embeddings.npy', embeddings)\n",
    "faiss.write_index(index, 'faiss_index.index')\n",
    "\n",
    "# Guardar datos procesados en un archivo pickle\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df.to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n",
    "\n",
    "# Cargar spaCy para español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    return ' '.join([token.lemma_ for token in doc])\n",
    "\n",
    "def query_faiss(query, top_k=5):\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    \n",
    "    with open('sentences.pkl', 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(top_k):\n",
    "        result = sentences[I[0][i]]\n",
    "        results.append({\n",
    "            'ID': result['ID'],\n",
    "            'Oracion_ID':result['Oracion_ID'],\n",
    "            'Oracion': result['Oracion'],\n",
    "            'Temas Clave': result['Temas Clave'],\n",
    "            'Partido': result['Partido'],\n",
    "            'CandidatoPresidente': result['CandidatoPresidente'],\n",
    "            'CandidatoVicePresidente': result['CandidatoVicePresidente'],\n",
    "            'ListaPolitica': result['ListaPolitica'],\n",
    "            'Distancia': D[0][i]\n",
    "        })\n",
    "    return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¿Cómo mejorar la eficiencia energética en la industria?\"\n",
    "results = query_faiss(query)\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto preprocesado: que propone daniel noboa para el fortalecimiento economico?\n",
      "Consulta procesada: proponer daniel noboa fortalecimiento economico\n",
      "Basándome en las declaraciones proporcionadas, se puede resumir que el tema principal de estas discursos y manifestaciones políticas es la promoción de políticas y acciones que benefician al país Ecuador, priorizando la soberanía nacional, la defensa del medio ambiente, la sostenibilidad económica y social, la seguridad ciudadana y la protección de los derechos humanos.\n",
      "\n",
      "Muchos de los partidos políticos mencionados en estas declaraciones, como Movimiento Creo, Movimiento Construye, Partido Sociedad Patriótica 21 de Enero, Partido Sociedad Unida Más Acción, Movimiento Amigo y Movimiento Centro Democrático, enfatizan la importancia de:\n",
      "\n",
      "1. **Desarrollo sostenible**: Promover políticas que equilibren el crecimiento económico con la protección ambiental y la conservación de los recursos naturales.\n",
      "2. **Soberanía nacional**: Priorizar la independencia y autonomía del país en sus relaciones internacionales, especialmente en la integración regional y en la cooperación internacional.\n",
      "3. **Seguridad ciudadana**: Asegurar la seguridad y la estabilidad social, protegiendo los derechos humanos y promoviendo la justicia social.\n",
      "4. **Economía equitativa**: Propiciar políticas económicas que favorezcan a la población en general, reduciendo la brecha entre riqueza y pobreza.\n",
      "5. **Desarrollo urbano y vial**: Mejorar el estado del transporte público y la infraestructura urbana para facilitar el acceso a servicios básicos y promover un crecimiento económico sostenible.\n",
      "\n",
      "En general, estos discursos reflejan una visión de desarrollo social y económico equilibrado, que priorice el bienestar de los ciudadanos y la protección del medio ambiente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "import ollama  # Importar la librería para usar Ollama\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV con las oraciones procesadas\n",
    "df = pd.read_csv('oraciones_procesadas_completo.csv', delimiter=';')\n",
    "\n",
    "# Cargar el archivo CSV con los datos de los partidos y candidatos\n",
    "\n",
    "candidatos_df = pd.read_csv('candidatos.csv', delimiter=';')  # Asegúrate de que el archivo correcto esté aquí\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si está disponible\n",
    "model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Cargar el índice FAISS previamente guardado\n",
    "index = faiss.read_index('faiss_index.index')\n",
    "\n",
    "# Cargar los datos de oraciones\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el modelo de spaCy para lematización en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    print(f\"Texto preprocesado: {text}\")\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Función para obtener la oración original sin procesar y sus datos adicionales\n",
    "def get_oracion_data_by_id(oracion_id):\n",
    "    oracion_row = df[df['Oracion_ID'] == oracion_id]\n",
    "    if not oracion_row.empty:\n",
    "        oracion = oracion_row.iloc[0]['Oracion']\n",
    "        temas_clave = oracion_row.iloc[0]['Temas Clave']\n",
    "        return oracion, temas_clave\n",
    "    return None, None\n",
    "\n",
    "def get_partido_data_by_id(id):\n",
    "    candidato_row = candidatos_df[candidatos_df['ID'] == id]  # Buscamos usando el 'ID'\n",
    "    if not candidato_row.empty:\n",
    "        partido = candidato_row.iloc[0]['Partido']\n",
    "        candidato_presidente = candidato_row.iloc[0]['CandidatoPresidente']\n",
    "        candidato_vicepresidente = candidato_row.iloc[0]['CandidatoVicePresidente']\n",
    "        lista_politica = candidato_row.iloc[0]['ListaPolitica']\n",
    "        return partido, candidato_presidente, candidato_vicepresidente, lista_politica\n",
    "    return None, None, None, None\n",
    "\n",
    "def query_faiss_ollama(query, top_k=50):  \n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    # Mostrar el tamaño de la consulta procesada\n",
    "    print(f\"Consulta procesada: {cleaned_query}\")\n",
    "    # Generar el embedding para la consulta\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    # Realizar la búsqueda en el índice FAISS\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "    # Procesar los resultados\n",
    "    resultados = []\n",
    "    for i in range(top_k):\n",
    "        # Obtener el ID de la oración correspondiente\n",
    "        oracion_id = sentences[I[0][i]]['Oracion_ID']\n",
    "\n",
    "        id= sentences[I[0][i]]['ID']\n",
    "        # Buscar la oración original y sus temas clave usando Oracion_ID\n",
    "        oracion_original, temas_clave = get_oracion_data_by_id(oracion_id)\n",
    "\n",
    "        # Buscar los datos del partido usando el ID del candidato\n",
    "        partido, candidato_presidente, candidato_vicepresidente, lista_politica = get_partido_data_by_id(id)\n",
    "\n",
    "        if oracion_original:  # Si se encuentra la oración original\n",
    "            resultado = (\n",
    "                f\"Oración: {oracion_original}, \"\n",
    "                f\"Temas Clave: {temas_clave}, \"\n",
    "                f\"Partido: {partido}, \"\n",
    "                f\"Presidente: {candidato_presidente}, \"\n",
    "                f\"Vicepresidente: {candidato_vicepresidente}, \"\n",
    "                f\"Lista Política: {lista_politica}\"\n",
    "            )\n",
    "            resultados.append(resultado)\n",
    "        \n",
    "    # Verificar si hemos obtenido resultados\n",
    "    if not resultados:\n",
    "        return \"No se encontraron oraciones relevantes para la consulta.\"\n",
    "\n",
    "    # Construir el prompt para Ollama con las oraciones obtenidas\n",
    "    prompt = f\"quiero que menciones los nombres de los Presidente,Vicepresidente,Lista Política y el partido de lo que he encontrado clasificalo:\\n\\n\" + \"\\n\".join(resultados) + \"\\n\\nGenera un resumen basado en estas declaraciones, explícalo.\"\n",
    "\n",
    "    # Generar la respuesta con Ollama\n",
    "    respuesta = ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "    return respuesta['message']['content']\n",
    "\n",
    "# Ejemplo de consulta para verificar el funcionamiento\n",
    "query = \"que propone daniel noboa para el fortalecimiento economico?\"\n",
    "respuesta = query_faiss_ollama(query)\n",
    "print(respuesta)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
