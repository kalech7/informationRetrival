{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Función para extraer texto del PDF excluyendo las primeras y últimas páginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extraído\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuración global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a imágenes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID único a cada oración\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID único para cada oración\n",
    "            oraciones_texto.append(oracion)  # Texto de la oración\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA SÍ\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo oraciones.csv procesado y limpiado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeración de página (ejemplo: 'Página 1', 'pág. 2', etc.)\n",
    "    text = re.sub(r'Página \\d+', '', text)\n",
    "    text = re.sub(r'pág\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfabéticos ni numéricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", y luego escribir las filas nuevamente\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  # Si no está vacío o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado y limpiado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alech/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer  # Usamos stemmer para español\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# Función para tokenizar, eliminar stopwords y aplicar stemming\n",
    "def preprocesar_texto(texto):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(texto.lower(), language='spanish')\n",
    "    \n",
    "    # Eliminar stopwords y caracteres no alfabéticos\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [word for word in tokens \n",
    "              if word.isalpha() \n",
    "              and word not in stop_words\n",
    "              and len(word) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", eliminar filas vacías y luego escribir las filas nuevamente\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: tokenizar, eliminar stopwords y aplicar stemming\n",
    "                        row['Oracion'] = preprocesar_texto(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  # Si no está vacío o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "input_csv = 'oraciones.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert y faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301a614fa1794f4aa240f733d1312ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/421 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('oraciones_procesadas.csv', delimiter=';')  # Cambia el nombre de tu archivo CSV si es necesario\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si está disponible\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "\n",
    "# Convertir los embeddings en un formato compatible con FAISS (float32)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un índice FAISS\n",
    "dimension = embeddings.shape[1]  # Dimensión de los embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # Índice basado en L2 (distancia euclidiana)\n",
    "index.add(embeddings)  # Agregar los embeddings al índice FAISS\n",
    "\n",
    "# Guardar los embeddings y el índice FAISS en archivos\n",
    "np.save('embeddings.npy', embeddings)  # Guardamos los embeddings en un archivo .npy\n",
    "faiss.write_index(index, 'faiss_index.index')  # Guardamos el índice FAISS en un archivo .index\n",
    "\n",
    "# Guardar los ID y las oraciones en un archivo .pkl para cargarlos fácilmente después\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df[['ID', 'Oracion']].to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ID                                            Oracion  \\\n",
      "0     1  recuper gratuid prepar ingres escuel polic nac...   \n",
      "1     1  promov prevencion violenci muj edad tempran tr...   \n",
      "2     1  establec mecan inmigr edad escol menor años as...   \n",
      "3     1  foment educ artist particip joven activ cultur...   \n",
      "4     2  construccion hospital escuel carreter provinci...   \n",
      "..   ..                                                ...   \n",
      "109  11  cumpl plan nacional reinsercion estudiantil mi...   \n",
      "110  11  contrat especial salud mental dentr escuel col...   \n",
      "111  11  promov profesionaliz artist popular vincul doc...   \n",
      "112  11  mism polit soberan concesion red fortalec empr...   \n",
      "113  15  signif sol cubr demand dej estudi pued acced s...   \n",
      "\n",
      "                                      Partido  \n",
      "0                REVOLUCIÓN CIUDADANA - RETO   \n",
      "1                REVOLUCIÓN CIUDADANA - RETO   \n",
      "2                REVOLUCIÓN CIUDADANA - RETO   \n",
      "3                REVOLUCIÓN CIUDADANA - RETO   \n",
      "4    PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA   \n",
      "..                                        ...  \n",
      "109                   PARTIDO UNIDAD POPULAR   \n",
      "110                   PARTIDO UNIDAD POPULAR   \n",
      "111                   PARTIDO UNIDAD POPULAR   \n",
      "112                   PARTIDO UNIDAD POPULAR   \n",
      "113                 PARTIDO SOCIAL CRISTIANO   \n",
      "\n",
      "[114 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para aplicar stemming a un texto\n",
    "def stem_text(text):\n",
    "    words = nltk.word_tokenize(text, language=\"spanish\")  # Tokenizar el texto\n",
    "    words_stemmed = [stemmer.stem(word) for word in words]  # Aplicar stemming\n",
    "    return \" \".join(words_stemmed)  # Reunir las palabras nuevamente en una oración\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_stemmed = stem_text(query)  # Aplicar stemming a la consulta\n",
    "\n",
    "    # Filtrar las oraciones que contengan términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_stemmed in stem_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, buscamos los partidos\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        print(df_resultados[['ID', 'Oracion', 'Partido']])\n",
    "    else:\n",
    "        print(\"No se encontraron oraciones que coincidan con la consulta.\")\n",
    "\n",
    "# Función para buscar el partido según el ID en `candidatos.csv`\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Ejemplo de consulta:\n",
    "query = \"escuelas\"\n",
    "buscar_por_consulta(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, so I've been given this query with a lot of information about different political parties and their educational policies in Ecuador. The user provided a list of IDs, each with an \"Oración\" (prayer or saying, but probably just a statement here) that outlines various issues and proposals related to education.\n",
      "\n",
      "First, I need to understand what the user is asking for. They want me to respond in natural language, summarizing the information clearly and understandably. So, my task isn't to list each point one by one but rather to provide an overview of the key themes across all these statements.\n",
      "\n",
      "Looking at the data, it's clear that multiple parties are addressing various aspects of education. For example, Partido Avanza is talking about intelligent educational models and teacher training, while others like UNIDAD POPULAR are focusing on school safety and reintegration plans for students. \n",
      "\n",
      "I should categorize the main topics to make the summary effective. These categories might include funding, infrastructure, teacher support, student welfare, innovation in education, access to higher education, combating desertion, and policy enforcement.\n",
      "\n",
      "Each party seems to have a unique angle but there are overlapping concerns like school safety, access to resources, and improving teaching conditions. It's important to highlight these overlaps as well as the distinct approaches each party is taking.\n",
      "\n",
      "I need to make sure the summary flows naturally, covering all these points without getting bogged down in too much detail. Keeping it concise yet comprehensive is key here.\n",
      "</think>\n",
      "\n",
      "De los datos proporcionados, se puede extraer una visión general de las propuestas y preocupaciones relacionadas con la educación en Ecuador, organizadas por partido político:\n",
      "\n",
      "### **Partido Avanza (ID 14)**\n",
      "- **Propuestas clave:**\n",
      "  - Rediseño del sistema educativo policial para incorporar nuevas atribuciones y competencias.\n",
      "  - Creación de planes de estudio centrados en habilidades esenciales, incluso en la obligatoriedad en todas las escuelas públicas y privadas.\n",
      "  - Implementación de campañas masivas de educación en salud, especialmente en comunidades rurales.\n",
      "  - Desarrollo de programas pilotos en escuelas que usan inteligencia artificial para personalizar la enseñanza según el ritmo de aprendizaje de los estudiantes.\n",
      "\n",
      "### **Partido Unidad Popular (ID 11)**\n",
      "- **Propuestas clave:**\n",
      "  - Mejora del equipamiento salarial docente y garantía de seguridad en escuelas, incluyendo protecciones contra acoso, vacunación y extorsión.\n",
      "  - Duplicación del plan anual de inversión para obras públicas como escuelas, hospitales y viviendas, mejorando la infraestructura.\n",
      "  - Implementación de un programa nacional de reinserción estudiantil para estudiantes que abandonaron estudios, con enfoque en garantizar el acceso a la educación.\n",
      "  - Contratación de profesionales en salud mental dentro de las escuelas y universidades.\n",
      "  - Fomento de la profesionalización y vinculación de docentes en procesos educativos.\n",
      "  - Mejora del sistema de concesiones y fortalecimiento empresarial en sectores públicos, como hidrocarburos, electricidad y telecomunicaciones.\n",
      "\n",
      "### **Partido Socialista Ecuatoriano (ID 12)**\n",
      "- **Propuestas clave:**\n",
      "  - Reconocimiento de la falta de servicios en escuelas y su impacto directo en la calidad educativa y el desarrollo cognitivo de los estudiantes.\n",
      "  - Enfatiza la necesidad de mejorar las condiciones de los docentes y asistentes escolares.\n",
      "\n",
      "### **Partido Sociedad Patriótica 21 de Enero (ID 10)**\n",
      "- **Propuestas clave:**\n",
      "  - Programas para garantizar el acceso a alimentos nutritivos en escuelas, mejorando el rendimiento académico y la salud de los estudiantes.\n",
      "  - Implementación de becas y convenios para desarrollar programas educativos en arte y deporte, incentivando el talento creativo desde temprana edad.\n",
      "  - Organización de campañas masivas para denunciar las condiciones inadecuadas de las escuelas en Ecuador.\n",
      "  - Uso de recursos logísticos y apoyo a la fuerza armada para garantizar nuevos recursos educativos.\n",
      "  - Promoción de programas educativos cívicos en las escuelas, utilizando herramientas digitales y recursose speciales.\n",
      "\n",
      "### **Otros Partidos**\n",
      "- **Partido Unitario Popular (ID 15):** Se enfoca en la importancia de garantizar el acceso a la educación superior para estudiantes que se quedarían sin estudios.\n",
      "- **Partido UNIDAD POPULAR:** Menciona políticas sobre soberanía, concesiones y combate a la corrupción, mejorando sectores como hidrocarburos, electricidad y telecomunicaciones.\n",
      "\n",
      "En resumen, los partidos políticos en Ecuador abarcan temas variados dentro de la educación, desde la mejora de infraestructura y salarios docentes hasta la implementación de tecnologías innovadoras y programas sociales para garantizar el acceso a la educación.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import ollama\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para aplicar stemming a un texto\n",
    "def stem_text(text):\n",
    "    words = nltk.word_tokenize(text, language=\"spanish\")  # Tokenizar el texto\n",
    "    words_stemmed = [stemmer.stem(word) for word in words]  # Aplicar stemming\n",
    "    return \" \".join(words_stemmed)  # Reunir las palabras nuevamente en una oración\n",
    "\n",
    "# Función para buscar el partido según el ID en `candidatos.csv`\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_stemmed = stem_text(query)  # Aplicar stemming a la consulta\n",
    "\n",
    "    # Filtrar las oraciones que contengan términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_stemmed in stem_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, agregamos el partido\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        return df_resultados[['ID', 'Oracion', 'Partido']]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función para generar respuesta en lenguaje natural con Ollama\n",
    "def generar_respuesta(query):\n",
    "    resultados = buscar_por_consulta(query)\n",
    "    \n",
    "    if resultados is not None:\n",
    "        # Convertimos los resultados en un texto para Ollama\n",
    "        texto_resultados = \"\\n\".join([f\"ID: {row['ID']}, Oración: {row['Oracion']}, Partido: {row['Partido']}\" \n",
    "                                      for _, row in resultados.iterrows()])\n",
    "\n",
    "        # Generamos la respuesta con Ollama\n",
    "        prompt = f\"\"\"\n",
    "        Basado en la consulta '{query}', encontré estas oraciones y sus respectivos partidos:\n",
    "        \n",
    "        {texto_resultados}\n",
    "\n",
    "        Responde en lenguaje natural resumiendo la información de manera clara y comprensible.\n",
    "        \"\"\"\n",
    "        respuesta = ollama.chat(model='deepseek-r1:14b', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "        return respuesta['message']['content']\n",
    "    else:\n",
    "        return \"No se encontraron resultados para tu consulta.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"escuelas\"\n",
    "respuesta_final = generar_respuesta(query)\n",
    "print(respuesta_final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
