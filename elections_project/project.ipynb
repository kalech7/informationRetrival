{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Función para extraer texto del PDF excluyendo las primeras y últimas páginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extraído\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuración global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a imágenes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID único a cada oración\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID único para cada oración\n",
    "            oraciones_texto.append(oracion)  # Texto de la oración\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA SÍ\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo oraciones.csv procesado, limpiado y ordenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeración de página (ejemplo: 'Página 1', 'pág. 2', etc.)\n",
    "    text = re.sub(r'Página \\d+', '', text)\n",
    "    text = re.sub(r'pág\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfabéticos ni numéricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", ordenar por ID y guardar\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  \n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Ordenar las filas por ID (conversión a int para evitar errores de ordenación)\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "\n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado, limpiado y ordenado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CSV actualizado con éxito. Se agregaron las columnas 'Temas Clave'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Cargar modelo de lenguaje en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el CSV con las oraciones\n",
    "df = pd.read_csv(\"oraciones.csv\", sep=\";\")\n",
    "\n",
    "# Lista ampliada de temas clave\n",
    "temas_relevantes = {\n",
    "    \"economía\", \"educación\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupción\", \"tecnología\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"política\", \"desarrollo\", \"energía\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovación\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversión\",\n",
    "    \"vivienda\", \"servicios públicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educación superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalización\", \"gestión pública\", \"comercio\",\n",
    "    \"cambio climático\", \"energías renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud pública\", \"gobernanza\", \"justicia social\", \"igualdad de género\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestación\", \"movilidad urbana\", \"biodiversidad\", \"educación financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnológica\",\n",
    "    \"educación digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnología\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"economía circular\",\n",
    "    \"ciudades inteligentes\", \"protección de datos\", \"energía solar\", \"transporte eléctrico\",\n",
    "    \"robotización\", \"computación cuántica\", \"espacio exterior\", \"protección ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad eléctrica\", \"alimentos orgánicos\", \"tecnología educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas autónomos\",\n",
    "    \"tecnología espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energía eólica\", \"tecnologías disruptivas\", \"energía geotérmica\",\n",
    "    \"nanotecnología\", \"microbioma\", \"bioeconomía\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energía limpia\", \"criptomonedas\", \"minería digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educación en línea\",\n",
    "    \"biomimicry\", \"ecoinnovación\", \"simulación computacional\", \"agricultura urbana\", \"cultivos inteligentes\"\n",
    "}\n",
    "\n",
    "# Función para extraer solo los temas clave\n",
    "def extraer_temas_clave(texto):\n",
    "    if pd.isna(texto):  # Manejar valores nulos\n",
    "        return \"\"\n",
    "\n",
    "    oraciones = sent_tokenize(texto, language=\"spanish\")  # Dividir en oraciones\n",
    "    temas_detectados = set()\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        # Identificar temas clave dentro de la oración\n",
    "        temas_detectados.update({tema for tema in temas_relevantes if tema in oracion.lower()})\n",
    "\n",
    "    # Retornar los temas clave detectados como una cadena separada por coma\n",
    "    return \", \".join(temas_detectados)\n",
    "\n",
    "# Aplicar la extracción de temas clave en cada fila del DataFrame\n",
    "df[\"Temas Clave\"] = df[\"Oracion\"].apply(extraer_temas_clave)\n",
    "\n",
    "# Guardar el nuevo CSV con la nueva columna 'Temas Clave' sin eliminar datos anteriores\n",
    "df.to_csv(\"oraciones_actualizado.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\" CSV actualizado con éxito. Se agregaron las columnas 'Temas Clave'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,lemmatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stopwords en español de NLTK\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Función para lematizar y eliminar stopwords\n",
    "def lematizar_y_eliminar_stopwords(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Lematizar y eliminar stopwords\n",
    "    return ' '.join([token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words])\n",
    "\n",
    "# Limpiar contenido eliminando puntuaciones y números\n",
    "def clean_content(texto):\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar puntuaciones\n",
    "    texto = re.sub(r'\\d+', '', texto)      # Eliminar números\n",
    "    return texto.lower()\n",
    "\n",
    "# Función para procesar el CSV\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: lematizar y eliminar stopwords\n",
    "                        row['Oracion'] = lematizar_y_eliminar_stopwords(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  # Si no está vacío o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion', 'Temas Clave'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "input_csv = 'oraciones_actualizado.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert y faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc66232ab6c94f179da7ad1e0d2f73a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n",
      "Resultados de la consulta:\n",
      "1. ID: 14 | Oración: mantenimiento modernización infraestructura mejorar eficiencia reducir pérdida sistema energético | Temas Clave: infraestructura, ciencia | Partido: PARTIDO AVANZA  (Distancia: 18.809154510498047)\n",
      "2. ID: 4 | Oración: eficiencia energético implementar política eficiencia energético sector clave industria transporte construcción promover uso tecnología reducir consumo energía | Temas Clave: energía, transporte, ciencia, tecnología, política, industria | Partido: MOVIMIENTO CENTRO DEMOCRÁTICO  (Distancia: 20.003599166870117)\n",
      "3. ID: 1 | Oración: intervenir vivienda promover eficiencia energético | Temas Clave: vivienda, ciencia | Partido: REVOLUCIÓN CIUDADANA - RETO  (Distancia: 20.277034759521484)\n",
      "4. ID: 8 | Oración: colaboración sector privado ser vital desarrollo sostenibilidad sistema energético | Temas Clave: desarrollo, sostenibilidad | Partido: MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_  (Distancia: 21.76897430419922)\n",
      "5. ID: 14 | Oración: campaña nacional eficiencia ahorro energético promover práctica eficiencia ahorro energético sector | Temas Clave: ciencia | Partido: PARTIDO AVANZA  (Distancia: 23.09795379638672)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV con las oraciones procesadas\n",
    "df = pd.read_csv('oraciones_procesadas.csv', delimiter=';')  # Cambia el nombre de tu archivo CSV si es necesario\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos y partidos\n",
    "candidatos_df = pd.read_csv('candidatos.csv', delimiter=';')  # Asegúrate de que contiene 'ID' y 'Nombre Partido'\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si está disponible\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "\n",
    "# Convertir los embeddings en un formato compatible con FAISS (float32)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un índice FAISS\n",
    "dimension = embeddings.shape[1]  # Dimensión de los embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # Índice basado en L2 (distancia euclidiana)\n",
    "index.add(embeddings)  # Agregar los embeddings al índice FAISS\n",
    "\n",
    "# Guardar los embeddings y el índice FAISS en archivos\n",
    "np.save('embeddings.npy', embeddings)  # Guardamos los embeddings en un archivo .npy\n",
    "faiss.write_index(index, 'faiss_index.index')  # Guardamos el índice FAISS en un archivo .index\n",
    "\n",
    "# Guardar los ID y las oraciones en un archivo .pkl para cargarlos fácilmente después\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df[['ID', 'Oracion', 'Temas Clave']].to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n",
    "\n",
    "# Cargar el modelo de spaCy para lematización en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para preprocesar el texto (limpieza y lematización)\n",
    "def preprocess_text(text):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    \n",
    "    # Eliminar stopwords y caracteres no alfabéticos\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    # Lematización usando spaCy\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Unir los tokens lematizados en una sola cadena\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Función para obtener el nombre del partido a partir del ID\n",
    "def get_partido_name(id):\n",
    "    partido_row = candidatos_df[candidatos_df['ID'] == id]\n",
    "    if not partido_row.empty:\n",
    "        return partido_row.iloc[0]['Partido']\n",
    "    return \"Partido no encontrado\"\n",
    "\n",
    "# Función para realizar una consulta\n",
    "def query_faiss(query, top_k=5):\n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_text(query)\n",
    "\n",
    "    # Generar el embedding para la consulta\n",
    "    query_embedding = model.encode([cleaned_query], device=device)\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "\n",
    "    # Realizar la búsqueda en FAISS\n",
    "    D, I = index.search(query_embedding, top_k)  # D son las distancias, I son los índices\n",
    "\n",
    "    # Obtener los resultados\n",
    "    with open('sentences.pkl', 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    print(\"Resultados de la consulta:\")\n",
    "    for i in range(top_k):\n",
    "        result = sentences[I[0][i]]\n",
    "        partido_nombre = get_partido_name(result['ID'])\n",
    "        print(f\"{i+1}. ID: {result['ID']} | Oración: {result['Oracion']} | Temas Clave: {result['Temas Clave']} | Partido: {partido_nombre} (Distancia: {D[0][i]})\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¿Cómo mejorar la eficiencia energética en la industria?\"\n",
    "query_faiss(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Alright, so I'm trying to help someone with their query about the economy and political parties. They provided a list of IDs, sentences, and corresponding parties. The task is to summarize this information in a clear and understandable way.\n",
      "\n",
      "First, I'll need to read through each entry carefully. It looks like each ID has multiple oraciones (sentences) related to economic topics, each tied to a specific partido político (political party). My goal is to group these sentences by their respective parties and then explain what each party is focusing on economically.\n",
      "\n",
      "Starting with ID 2, all the sentences are from \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA.\" The topics here seem to be about economic development, maybe creating special zones or boards for economic growth, similar to Singapore's model. So I'll note that this party is talking about economic development and special zones.\n",
      "\n",
      "Next, ID 4 is from \"MOVIMIENTO CENTRO DEMOCRÁTICO.\" The sentences mention things like economic-social inclusion and policies aimed at reducing inequality and poverty. It seems like they're focusing on social aspects of the economy, so I'll highlight their focus on social inclusion and reducing disparities.\n",
      "\n",
      "ID 5 corresponds to \"MOVIMIENTO CONSTRUYE.\" Their sentences discuss foreign investment in sectors like oil production. They seem concerned with economic stability through investments and job creation, especially in industries that are key to the national economy. So I'll note their emphasis on foreign investment and economic recovery through specific programs.\n",
      "\n",
      "ID 6 is linked to \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES.\" The sentences talk about rural economic opportunities and employment growth. They seem focused on fostering economic growth in rural areas, possibly through job creation and sustainable practices. I'll mention their emphasis on rural economy and job opportunities.\n",
      "\n",
      "ID 10 is from \"PARTIDO SOCIEDAD PATRIÓTICA 21 DE ENERO.\" Their sentences discuss tourism promotion and security to attract foreign tourists, referencing a report from the World Economic Forum. It looks like they're addressing regional economic stability through tourism and improving perceptions of safety. I'll note their focus on tourism and regional stability.\n",
      "\n",
      "ID 16 is tied to \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK.\" The sentences here talk about public funds aimed at supporting small family economies, especially in agriculture, and fostering food sovereignty. They seem focused on sustainable economic practices and helping small farmers adapt to climate change. I'll highlight their support for small-scale farming and food sovereignty.\n",
      "\n",
      "ID 11 is from \"PARTIDO UNIDAD POPULAR.\" Their sentences mention a multi-year government plan aimed at recovering the economy by generating specific jobs and objectives, suggesting they're planning long-term economic recovery strategies. I'll note their focus on economic recovery through strategic plans.\n",
      "\n",
      "Lastly, ID 15 corresponds to \"PARTIDO SOCIAL CRISTIANO.\" The sentences discuss implementing policies and programs to promote formalization in the labor market, particularly in specialized areas. It seems like they're pushing for more structured and regulated labor practices. I'll mention their emphasis on formalizing the economy through specific policies.\n",
      "\n",
      "Putting it all together, each political party has distinct economic focuses ranging from development strategies and social inclusion to rural growth, tourism, small-scale agriculture, long-term recovery, and labor market formalization. My summary should clearly outline these points in a natural, easy-to-understand manner.\n",
      "</think>\n",
      "\n",
      "Basado en la información proporcionada, se puede observar cómo diferentes partidos políticos abordan temas relacionados con la economía en Ecuador:\n",
      "\n",
      "1. **PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN (SUMA)**:\n",
      "   - Se enfoca en el desarrollo económico y la creación de zonas económicas especiales, similar al modelo de Singapur.\n",
      "   - Ejemplos incluyen la implementación de un \"Economic Development Board\" para promover inversiones y fomentar el desarrollo económico.\n",
      "\n",
      "2. **MOVIMIENTO CENTRO DEMOCRÁTICO**:\n",
      "   - Aborda temas de inclusión social y reducción de desigualdades a través de políticas económicas y sociales.\n",
      "   - Busca implementar modelos de desarrollo que garanticen la equidad y el acceso a recursos económicos.\n",
      "\n",
      "3. **MOVIMIENTO CONSTRUYE**:\n",
      "   - Discute sobre la importancia de las inversiones extranjeras directas y su impacto en la economía nacional, especialmente en sectores clave como la producción petrolera.\n",
      "   - Se menciona la necesidad de promover el empleo y garantizar el crecimiento económico a través de planes claros.\n",
      "\n",
      "4. **MOVIMIENTO CREO (CREANDO OPORTUNIDADES)**:\n",
      "   - Se centra en fomentar el desarrollo económico rural y la generación de empleos.\n",
      "   - Busca impulsar el crecimiento económico mediante la creación de oportunidades laborales y la explotación de recursos naturales.\n",
      "\n",
      "5. **PARTIDO SOCIEDAD PATRIÓTICA (21 DE ENERO)**:\n",
      "   - Aborda temas de seguridad turística y la necesidad de promover la economía regional.\n",
      "   - Se menciona la importancia de mejorar la percepción de seguridad para atraer más turistas extranjeros.\n",
      "\n",
      "6. **MOVIMIENTO DE UNIDAD PLURINACIONAL (PACHAKUTIK)**:\n",
      "   - Fomenta el apoyo a las economías familiares campesinas y la soberanía alimentaria.\n",
      "   - Propone subsidios directos para garantizar la seguridad alimentaria y adaptar a los cambios climáticos.\n",
      "\n",
      "7. **PARTIDO UNIDAD POPULAR**:\n",
      "   - Discute sobre planes plurianuales para recuperar la economía, con énfasis en generar empleos dignos y alcanzar objetivos específicos.\n",
      "   - Se enfoca en políticas económicas que promuevan el crecimiento sostenible.\n",
      "\n",
      "8. **PARTIDO SOCIAL CRISTIANO**:\n",
      "   - Busca implementar políticas y programas para formalizar la economía, especialmente en el mercado laboral.\n",
      "   - Promueve la especialización en áreas específicas del mercado laboral para mejorar la productividad y la estabilidad económica.\n",
      "\n",
      "Cada partido político aborda temas económicos de manera distinta, adaptando sus propuestas a sus visiones y objetivos políticos.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "import ollama\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar el modelo de spaCy en español para lematización\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para aplicar lematización a un texto\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)  # Procesar el texto con spaCy\n",
    "    lemmatized_words = [token.lemma_ for token in doc]  # Obtener los lemas de cada palabra\n",
    "    return \" \".join(lemmatized_words)  # Reunir las palabras lematizadas en una oración\n",
    "\n",
    "# Función para buscar el partido según el ID en candidatos.csv\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_lemmatized = lemmatize_text(query)  # Aplicar lematización a la consulta\n",
    "\n",
    "    # Filtrar las oraciones que contengan términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_lemmatized in lemmatize_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, agregamos el partido\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        return df_resultados[['ID', 'Oracion', 'Partido']]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función para generar respuesta en lenguaje natural con Ollama\n",
    "def generar_respuesta(query):\n",
    "    resultados = buscar_por_consulta(query)\n",
    "    \n",
    "    if resultados is not None:\n",
    "        # Convertimos los resultados en un texto para Ollama\n",
    "        texto_resultados = \"\\n\".join([f\"ID: {row['ID']}, Oración: {row['Oracion']}, Partido: {row['Partido']}\" \n",
    "                                      for _, row in resultados.iterrows()])\n",
    "\n",
    "        # Generamos la respuesta con Ollama\n",
    "        prompt = f\"\"\"\n",
    "        Basado en la consulta '{query}', encontré estas oraciones y sus respectivos partidos:\n",
    "        \n",
    "        {texto_resultados}\n",
    "\n",
    "        Responde en lenguaje natural resumiendo la información de manera clara y comprensible.\n",
    "        \"\"\"\n",
    "        respuesta = ollama.chat(model='deepseek-r1:14b', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "        return respuesta['message']['content']\n",
    "    else:\n",
    "        return \"No se encontraron resultados para tu consulta.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"economia\"\n",
    "respuesta_final = generar_respuesta(query)\n",
    "print(respuesta_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alech/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Bueno, el usuario me ha proporcionado una información relevante que extrajo de la pregunta \"¿Cómo afecta la economía al país?\". La información incluye un ID (2), una oración en español, y menciona a un partido político llamado \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA\".\n",
      "\n",
      "Primero, entiendo que el usuario quiere una lista completa de candidatos con esa misma información. Sin embargo, solo ha proporcionado un ejemplo con ID 2 y ORACIÓN, así como el nombre del partido.\n",
      "\n",
      "Para generar una respuesta adecuada, necesito asumir que hay más candidatos y posiblemente más IDs y oraciones relacionadas con la economía y su impacto en el país. Imagino que cada candidato podría tener un ID único, una oración relevante y pertenecer a un partido político.\n",
      "\n",
      "Después de realizar esta asunción, procedo a crear una lista de candidatos imaginaria, asignando IDs consecutivos (desde 1 en adelante), creando oraciones relevantes que conectan la economía con el país, y atribuyendo cada uno al partido \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA\" como se mencionó.\n",
      "\n",
      "Es importante asegurarme de que las oraciones sean variadas y reflejen diferentes aspectos de cómo la economía puede afectar al país. Además, estructuro cada candidato en un formato consistente para mayor claridad.\n",
      "\n",
      "Finalmente, agrego una nota al final de la respuesta pidiendo más detalles si el usuario tiene una fuente específica o más información que pueda compartir, lo cual ayudaría a proporcionar una respuesta más precisa y personalizada.\n",
      "</think>\n",
      "\n",
      "Claro, basándome en la información proporcionada, aquí tienes una lista imaginaria de candidatos con los mismos parámetros:\n",
      "\n",
      "1.  \n",
      "   - **ID:** 1  \n",
      "   - **Oración:** \"El crecimiento económico sostenible es clave para el desarrollo del país y mejorar la calidad de vida de los ciudadanos.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "2.  \n",
      "   - **ID:** 3  \n",
      "   - **Oración:** \"Necesitamos implementar políticas económicas que incentiven el emprendimiento y reduzcan la desigualdad social.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "3.  \n",
      "   - **ID:** 4  \n",
      "   - **Oración:** \"La estabilidad financiera es fundamental para atraer inversiones y fortalecer la economía del país.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "4.  \n",
      "   - **ID:** 5  \n",
      "   - **Oración:** \"El sector fintech tiene un gran potencial para impulsar el crecimiento económico y modernizar el sistema financiero ecuatoriano.\"  \n",
      "   - **Partido:** PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA  \n",
      "\n",
      "Si tienes más información o una fuente específica, podría ayudarte a proporcionar una respuesta más precisa.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pickle\n",
    "import nltk\n",
    "import ollama\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "# Descargar recursos de NLTK si es necesario\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el stemmer en español y cargar stopwords\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "stop_words = set(stopwords.words(\"spanish\"))\n",
    "\n",
    "# Cargar los embeddings y el índice FAISS\n",
    "embeddings = np.load('embeddings.npy')  \n",
    "index = faiss.read_index('faiss_index.index')  \n",
    "\n",
    "# Cargar las oraciones desde el archivo .pkl\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos\n",
    "df_candidatos = pd.read_csv('candidatos.csv', sep=';')  # Asegúrate de que el separador sea correcto\n",
    "\n",
    "# Función para preprocesar el texto: tokenización, eliminación de stopwords y stemming\n",
    "def preprocess_text(text):\n",
    "    words = word_tokenize(text, language=\"spanish\")  # Tokenizar el texto\n",
    "    words_filtered = [word for word in words if word.isalnum() and word.lower() not in stop_words]  # Eliminar stopwords\n",
    "    words_stemmed = [stemmer.stem(word) for word in words_filtered]  # Aplicar stemming\n",
    "    return \" \".join(words_stemmed)  # Reunir las palabras nuevamente en una oración\n",
    "\n",
    "# Función para obtener el partido según el ID en candidatos.csv\n",
    "def obtener_partido(id_):\n",
    "    partido = df_candidatos.loc[df_candidatos['ID'] == id_, 'Partido']\n",
    "    return partido.values[0] if not partido.empty else \"No encontrado\"\n",
    "\n",
    "# Función para buscar oraciones que contienen términos similares a la consulta\n",
    "def buscar_por_consulta(query):\n",
    "    query_processed = preprocess_text(query)  # Preprocesar la consulta del usuario\n",
    "\n",
    "    # Filtrar oraciones con términos relacionados con la consulta procesada\n",
    "    oraciones_filtradas = [\n",
    "        oracion for oracion in sentences if query_processed in preprocess_text(oracion['Oracion'])\n",
    "    ]\n",
    "\n",
    "    # Crear un DataFrame con los resultados\n",
    "    df_resultados = pd.DataFrame(oraciones_filtradas)\n",
    "\n",
    "    # Si hay resultados, agregar la columna de partidos\n",
    "    if not df_resultados.empty:\n",
    "        df_resultados['Partido'] = df_resultados['ID'].map(lambda id_: obtener_partido(id_))\n",
    "        return df_resultados[['ID', 'Oracion', 'Partido']]\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Función para generar respuesta en lenguaje natural con Ollama\n",
    "def generar_respuesta(query):\n",
    "    resultados = buscar_por_consulta(query)\n",
    "    \n",
    "    if resultados is not None:\n",
    "        # Convertimos los resultados en un texto para Ollama\n",
    "        texto_resultados = \"\\n\".join([f\"ID: {row['ID']}, Oración: {row['Oracion']}, Partido: {row['Partido']}\" \n",
    "                                      for _, row in resultados.iterrows()])\n",
    "\n",
    "        # Generamos la respuesta con Ollama\n",
    "        prompt = f\"\"\"\n",
    "        Basado en '{query}', encontré la siguiente información relevante:\n",
    "\n",
    "        {texto_resultados}\n",
    "\n",
    "        dame la lista entera de canditados con esa informacion\n",
    "        \"\"\"\n",
    "        respuesta = ollama.chat(model='deepseek-r1:14b', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "        return respuesta['message']['content']\n",
    "    else:\n",
    "        return \"No se encontraron resultados para tu consulta.\"\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¿Cómo afecta la economía al país?\"\n",
    "respuesta_final = generar_respuesta(query)\n",
    "print(respuesta_final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
