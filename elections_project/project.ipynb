{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Función para extraer texto del PDF excluyendo las primeras y últimas páginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extraído\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name, cleaned_content])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre', 'Contenido'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## documentos escaneados o protegidos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                     Nombre Contenido\n",
      "10  11    PARTIDO UNIDAD POPULAR        NaN\n",
      "12  13  MOVIMIENTO DEMOCRACIA SÍ        NaN\n",
      "14  15  PARTIDO SOCIAL CRISTIANO        NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('candidatos.csv', sep=';')\n",
    "\n",
    "# Lista de IDs que quieres buscar\n",
    "ids_a_buscar = [11, 13, 15]\n",
    "\n",
    "# Filtrar las filas donde el valor de la columna 'ID' esté en la lista\n",
    "filas = df[df['ID'].isin(ids_a_buscar)]\n",
    "\n",
    "# Mostrar las filas\n",
    "print(filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "csv.field_size_limit(1000000)\n",
    "# Configuración global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"candidatos.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a imágenes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        filas_existentes = []\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                filas_existentes = list(reader)\n",
    "        \n",
    "        # Añadir la nueva fila con los datos del PDF\n",
    "        filas_existentes.append({\n",
    "            'ID': id_asignado,\n",
    "            'Nombre': nombre_doc,\n",
    "            'Contenido': contenido\n",
    "        })\n",
    "        \n",
    "        # Ordenar las filas por el campo 'ID'\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "        \n",
    "        # Escribir las filas ordenadas nuevamente en el CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=columns, delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA SÍ\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\" Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## documentos ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    ID                    Nombre  \\\n",
      "10  11    PARTIDO UNIDAD POPULAR   \n",
      "12  13  MOVIMIENTO DEMOCRACIA SÍ   \n",
      "13  13  MOVIMIENTO DEMOCRACIA SÍ   \n",
      "15  15  PARTIDO SOCIAL CRISTIANO   \n",
      "16  15  PARTIDO SOCIAL CRISTIANO   \n",
      "\n",
      "                                            Contenido  \n",
      "10  Unir al Pueblo para ser gobierno  12  Unidad P...  \n",
      "12  PROGRAMA DE GOBIERNO 2025 - 2029  COMPROMISO P...  \n",
      "13  PROGRAMA DE GOBIERNO 2025 - 2029  COMPROMISO P...  \n",
      "15  PLAN DE TRABAJO PARTIDO SOCIAL CRISTIANO LISTA...  \n",
      "16  PLAN DE TRABAJO PARTIDO SOCIAL CRISTIANO LISTA...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Cargar el archivo CSV\n",
    "df = pd.read_csv('candidatos.csv', sep=';')\n",
    "\n",
    "# Lista de IDs que quieres buscar\n",
    "ids_a_buscar = [11,13,15]\n",
    "\n",
    "# Filtrar las filas donde el valor de la columna 'ID' esté en la lista\n",
    "filas = df[df['ID'].isin(ids_a_buscar)]\n",
    "\n",
    "# Mostrar las filas\n",
    "print(filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo candidatos.csv procesado y limpiado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeración de página (ejemplo: 'Página 1', 'pág. 2', etc.)\n",
    "    text = re.sub(r'Página \\d+', '', text)\n",
    "    text = re.sub(r'pág\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfabéticos ni numéricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Contenido\", y luego escribir las filas nuevamente\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Contenido\"\n",
    "                    row['Contenido'] = clean_content(row['Contenido'])\n",
    "                    filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Nombre', 'Contenido'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado y limpiado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "limpiar_y_guardar_csv('candidatos.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/alech/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesamiento completado. Archivo guardado en: candidatos_procesados.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer  # Usamos stemmer para español\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el stemmer en español\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def preprocesar_texto(texto):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(texto.lower(), language='spanish')\n",
    "    \n",
    "    # Eliminar stopwords y caracteres no alfabéticos\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [word for word in tokens \n",
    "              if word.isalpha() \n",
    "              and word not in stop_words\n",
    "              and len(word) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def procesar_csv(input_csv, output_csv):\n",
    "    with open(input_csv, 'r', encoding='utf-8-sig') as entrada, \\\n",
    "         open(output_csv, 'w', newline='', encoding='utf-8-sig') as salida:\n",
    "        \n",
    "        lector = csv.DictReader(entrada, delimiter=';')\n",
    "        campos = lector.fieldnames\n",
    "        \n",
    "        escritor = csv.DictWriter(salida, fieldnames=campos, delimiter=';')\n",
    "        escritor.writeheader()\n",
    "        \n",
    "        for fila in lector:\n",
    "            if 'Contenido' in fila:\n",
    "                # Procesar el contenido: tokenizar, eliminar stopwords y aplicar stemming\n",
    "                fila['Contenido'] = preprocesar_texto(fila['Contenido'])\n",
    "            escritor.writerow(fila)\n",
    "\n",
    "# Ejecutar el procesamiento\n",
    "input_csv = 'candidatos.csv'\n",
    "output_csv = 'candidatos_procesados.csv'\n",
    "procesar_csv(input_csv, output_csv)\n",
    "\n",
    "print(f\"Procesamiento completado. Archivo guardado en: {output_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando dispositivo: cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d738e4ca234a9781c343e4e36966ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/364 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33268fa41d54d96aba1160564fa8e18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/648 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc265973762447f97fcae24b9aaf638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/242k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d912fe69f50c41828801df2dca4ea26b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/480k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d66c508985a44e1b195f4dd67ba0951",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c7361471aeb4c7ba932de828ea6a4ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 2/2 [00:00<00:00,  3.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings generados: (18, 768)\n",
      "DataFrame guardado en 'candidatos_con_embeddings.pkl'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0775cedd4de640cb8f97561839936e2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Verificar GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "# Cargar datos\n",
    "df = pd.read_csv('candidatos_procesados.csv', delimiter=';', encoding='utf-8-sig')\n",
    "textos = df['Contenido'].astype(str).tolist()\n",
    "\n",
    "# Cargar modelo BERT en español\n",
    "model_name = \"dccuchile/bert-base-spanish-wwm-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Función optimizada para embeddings con GPU\n",
    "def get_bert_embeddings(batch_texts):\n",
    "    inputs = tokenizer(\n",
    "        batch_texts,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Obtener el promedio de los hidden states (ignorando padding tokens)\n",
    "    attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "    embeddings = (outputs.last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "# Procesamiento por lotes\n",
    "batch_size = 16  # Ajustar según tu GPU\n",
    "embeddings = []\n",
    "\n",
    "for i in tqdm(range(0, len(textos), batch_size)):\n",
    "    batch = textos[i:i+batch_size]\n",
    "    embeddings_batch = get_bert_embeddings(batch)\n",
    "    embeddings.append(embeddings_batch)\n",
    "\n",
    "embeddings = np.concatenate(embeddings)\n",
    "\n",
    "# Guardar embeddings y dataframe actualizado\n",
    "df['embedding'] = list(embeddings)\n",
    "df.to_pickle('candidatos_con_embeddings.pkl')\n",
    "\n",
    "print(f\"Embeddings generados: {embeddings.shape}\")\n",
    "print(\"DataFrame guardado en 'candidatos_con_embeddings.pkl'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dccuchile/bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos cargados: 18 registros\n",
      "Índice HNSW construido con 18 vectores\n",
      "Índice guardado en candidatos_faiss.index\n",
      "\n",
      "Resultados de búsqueda:\n",
      "                                          Nombre  \\\n",
      "14                               PARTIDO AVANZA    \n",
      "8   MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN    \n",
      "11               PARTIDO SOCIALISTA ECUATORIANO    \n",
      "\n",
      "                                            Contenido     score  \n",
      "14  propuest gran devolu diagnost pais republ ecua...  0.468735  \n",
      "8   ntroduccion coincid grup ciudadan mism interes...  0.455041  \n",
      "11  introduccion present plan trabaj part social c...  0.438196  \n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. Configuración inicial\n",
    "class FAISSManager:\n",
    "    def __init__(self, model_name=\"dccuchile/bert-base-spanish-wwm-cased\"):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(self.device)\n",
    "        self.index = None\n",
    "        self.df = None\n",
    "        self.embeddings = None\n",
    "        \n",
    "        # Configuración FAISS\n",
    "        self.dimension = 768  # Dimensión de BERT-base\n",
    "        self.nlist = 100  # Número de clusters para índices IVF\n",
    "        self.quantizer = None\n",
    "        self.index_type = \"HNSW\"  # Opciones: FlatL2, IVF, HNSW\n",
    "\n",
    "    # 2. Cargar datos\n",
    "    def load_data(self, pkl_path='candidatos_con_embeddings.pkl'):\n",
    "        self.df = pd.read_pickle(pkl_path)\n",
    "        self.embeddings = np.stack(self.df['embedding'].values).astype('float32')\n",
    "        print(f\"Datos cargados: {len(self.df)} registros\")\n",
    "    \n",
    "    # 3. Crear índice FAISS\n",
    "    def build_index(self, normalize=True):\n",
    "        if normalize:\n",
    "            faiss.normalize_L2(self.embeddings)\n",
    "        \n",
    "        if self.index_type == \"FlatL2\":\n",
    "            self.index = faiss.IndexFlatL2(self.dimension)\n",
    "        elif self.index_type == \"IVF\":\n",
    "            self.quantizer = faiss.IndexFlatL2(self.dimension)\n",
    "            self.index = faiss.IndexIVFFlat(self.quantizer, self.dimension, self.nlist)\n",
    "            self.index.train(self.embeddings)\n",
    "        elif self.index_type == \"HNSW\":\n",
    "            self.index = faiss.IndexHNSWFlat(self.dimension, 32)  # 32 enlaces por nodo\n",
    "        \n",
    "        self.index.add(self.embeddings)\n",
    "        print(f\"Índice {self.index_type} construido con {self.index.ntotal} vectores\")\n",
    "    \n",
    "    # 4. Guardar/Cargar índice\n",
    "    def save_index(self, path='candidatos_faiss.index'):\n",
    "        faiss.write_index(self.index, path)\n",
    "        print(f\"Índice guardado en {path}\")\n",
    "    \n",
    "    def load_index(self, path='candidatos_faiss.index'):\n",
    "        self.index = faiss.read_index(path)\n",
    "        print(f\"Índice cargado desde {path}\")\n",
    "    \n",
    "    # 5. Búsqueda de similitud\n",
    "    def search(self, query, k=5, use_cosine=True):\n",
    "        # Generar embedding para la consulta\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        \n",
    "        if use_cosine:\n",
    "            faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        distances, indices = self.index.search(query_embedding, k)\n",
    "        \n",
    "        resultados = self.df.iloc[indices[0]].copy()\n",
    "        resultados['score'] = distances[0] if not use_cosine else 1 - distances[0]\n",
    "        return resultados.sort_values('score', ascending=not use_cosine)\n",
    "    \n",
    "    # 6. Función de embedding\n",
    "    def _get_embedding(self, text):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=128,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        \n",
    "        attention_mask = inputs['attention_mask'].unsqueeze(-1)\n",
    "        embedding = (outputs.last_hidden_state * attention_mask).sum(dim=1) / attention_mask.sum(dim=1)\n",
    "        return embedding.cpu().numpy().astype('float32')\n",
    "    \n",
    "    # 7. Añadir nuevos elementos\n",
    "    def add_to_index(self, new_texts):\n",
    "        new_embeddings = np.concatenate([self._get_embedding(text) for text in new_texts])\n",
    "        if faiss.get_num_gpus() > 0:\n",
    "            self.index = faiss.index_cpu_to_gpu(faiss.StandardGpuResources(), 0, self.index)\n",
    "        self.index.add(new_embeddings)\n",
    "        print(f\"Añadidos {len(new_texts)} nuevos vectores al índice\")\n",
    "\n",
    "# Uso completo del sistema\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicializar manager\n",
    "    faiss_manager = FAISSManager()\n",
    "    \n",
    "    # Cargar datos existentes\n",
    "    faiss_manager.load_data()\n",
    "    \n",
    "    # Construir o cargar índice\n",
    "    rebuild_index = True  # Cambiar a False para cargar existente\n",
    "    if rebuild_index:\n",
    "        faiss_manager.build_index(normalize=True)\n",
    "        faiss_manager.save_index()\n",
    "    else:\n",
    "        faiss_manager.load_index()\n",
    "    \n",
    "    # Ejemplo de búsqueda\n",
    "    query = \"propuestas educativas innovadoras\"\n",
    "    resultados = faiss_manager.search(query, k=3)\n",
    "    \n",
    "    print(\"\\nResultados de búsqueda:\")\n",
    "    print(resultados[['Nombre', 'Contenido', 'score']])  # Ajustar columnas según tu CSV\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
