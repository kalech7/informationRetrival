{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procesamiento de pdfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# columna id, partido politico,content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: candidatos.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"candidatos.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\"},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\"},\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        data.append([file_id, processed_name])\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los datos nuevos\n",
    "df_new = pd.DataFrame(data, columns=['ID', 'Nombre'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_new\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos agregados al archivo CSV: oraciones.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import pdfplumber\n",
    "\n",
    "# Directorio donde están los archivos PDF\n",
    "pdf_directory = \"./data/\"\n",
    "output_csv = \"oraciones.csv\"\n",
    "\n",
    "# Lista de diccionarios específicos a procesar\n",
    "file_parameters = [\n",
    "    {\"file_name\": \"REVOLUCIÓN CIUDADANA - RETO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 8},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD UNIDA MÁS ACCIÓN, SUMA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 7},\n",
    "    {\"file_name\": \"PARTIDO IZQUIERDA DEMOCRÁTICA _Plan de trabajo_.pdf\",\"exclude_pages_start\": 5},\n",
    "    {\"file_name\": \"MOVIMIENTO CENTRO DEMOCRÁTICO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CONSTRUYE _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO CREO, CREANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO AMIGO, ACCIÓN MOVILIZADORA INDEPENDIENTE GENERANDO OPORTUNIDADES _Plan de trabajo_.pdf\", \"exclude_pages_start\": 4},\n",
    "    {\"file_name\": \"MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"MOVIMIENTO ACCION DEMOCRATICA NACIONAL, ADN _Plan de trabajo_.pdf\", \"exclude_pages_start\": 3},\n",
    "    {\"file_name\": \"PARTIDO SOCIEDAD PATRIÓTICA  21 DE ENERO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIALISTA ECUATORIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO AVANZA _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\", \"exclude_pages_start\": 2},\n",
    "    {\"file_name\": \"MOVIMIENTO DE UNIDAD PLURINACIONAL PACHAKUTIK _Plan de trabajo_.pdf\", \"exclude_pages_start\": 1}\n",
    "]\n",
    "\n",
    "# Función para obtener el último ID del archivo CSV\n",
    "def get_last_id(csv_path):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return 1\n",
    "    df = pd.read_csv(csv_path, sep=\"|\", encoding=\"utf-8\")\n",
    "    if df.empty:\n",
    "        return 1\n",
    "    return df['ID'].iloc[-1] + 1\n",
    "\n",
    "# Función para extraer texto del PDF excluyendo las primeras y últimas páginas\n",
    "def extract_text_excluding_pages(pdf_path, exclude_pages_start, exclude_pages_end=1):\n",
    "    extracted_text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for i in range(exclude_pages_start, len(pdf.pages) - exclude_pages_end):\n",
    "            page_text = pdf.pages[i].extract_text()\n",
    "            if page_text:\n",
    "                extracted_text += page_text + \"\\n\"\n",
    "    return extracted_text.strip()\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "    \n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Función para dividir el texto en oraciones\n",
    "def dividir_oraciones_por_id(text, text_id):\n",
    "    delimitadores = '.'\n",
    "    oraciones = []\n",
    "    oracion_actual = \"\"\n",
    "    for char in text:\n",
    "        oracion_actual += char\n",
    "        if char in delimitadores:\n",
    "            oraciones.append(oracion_actual.strip())\n",
    "            oracion_actual = \"\"\n",
    "    if oracion_actual:  # Si hay algo restante\n",
    "        oraciones.append(oracion_actual.strip())\n",
    "    \n",
    "    # Crear una lista de tuplas con id y oraciones\n",
    "    return [(text_id, i, oracion) for i, oracion in enumerate(oraciones, start=1)]\n",
    "\n",
    "# Obtener el ID inicial\n",
    "file_id = get_last_id(output_csv)\n",
    "\n",
    "# Crear una lista para almacenar los datos\n",
    "data = []\n",
    "\n",
    "# Recorrer la lista de diccionarios específicos\n",
    "for file_param in file_parameters:\n",
    "    file_name = file_param[\"file_name\"]\n",
    "    exclude_pages_start = file_param[\"exclude_pages_start\"]\n",
    "\n",
    "    # Construir la ruta completa del archivo\n",
    "    pdf_path = os.path.join(pdf_directory, file_name)\n",
    "\n",
    "    # Verificar si el archivo existe\n",
    "    if os.path.exists(pdf_path):\n",
    "        # Procesar el nombre del archivo\n",
    "        processed_name = file_name.replace(\"_Plan de trabajo_\", \"\").replace(\".pdf\", \"\")\n",
    "\n",
    "        # Extraer el contenido del PDF\n",
    "        content = extract_text_excluding_pages(pdf_path, exclude_pages_start=exclude_pages_start)\n",
    "\n",
    "        # Limpiar el contenido extraído\n",
    "        cleaned_content = clean_content(content)\n",
    "\n",
    "        # Dividir el contenido en oraciones\n",
    "        oraciones = dividir_oraciones_por_id(cleaned_content, file_id)\n",
    "\n",
    "        # Agregar las oraciones a la lista de datos\n",
    "        data.extend(oraciones)\n",
    "\n",
    "        # Incrementar el ID\n",
    "        file_id += 1\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {file_name}\")\n",
    "\n",
    "# Crear un DataFrame a partir de las oraciones\n",
    "df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "\n",
    "# Verificar si el archivo CSV ya existe\n",
    "if os.path.exists(output_csv):\n",
    "    # Leer el archivo CSV existente\n",
    "    df_existing = pd.read_csv(output_csv, sep=\"|\", encoding=\"utf-8\")\n",
    "    # Concatenar los datos nuevos con los existentes\n",
    "    df_combined = pd.concat([df_existing, df_oraciones], ignore_index=True)\n",
    "else:\n",
    "    df_combined = df_oraciones\n",
    "\n",
    "# Guardar el DataFrame combinado en el archivo CSV con delimitador \";\"\n",
    "df_combined.to_csv(output_csv, sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "print(f\"Datos agregados al archivo CSV: {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alech/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf procesado (ID 11)\n",
      "MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf procesado (ID 13)\n",
      "PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf procesado (ID 15)\n",
      "\n",
      "Proceso completado.\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import csv\n",
    "import os\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar el recurso necesario para tokenizar oraciones\n",
    "nltk.download('punkt')\n",
    "\n",
    "csv.field_size_limit(1000000)\n",
    "\n",
    "# Configuración global\n",
    "pdf_directory = \"./data/\"\n",
    "csv_file = \"oraciones.csv\"\n",
    "columns = ['ID', 'Nombre', 'Contenido']\n",
    "\n",
    "# Configurar Tesseract para Fedora\n",
    "pytesseract.pytesseract.tesseract_cmd = '/usr/bin/tesseract'\n",
    "\n",
    "def procesar_pdf(ruta_pdf, id_asignado, nombre_doc):\n",
    "    try:\n",
    "        # Convertir PDF a imágenes\n",
    "        images = convert_from_path(ruta_pdf, dpi=300)\n",
    "        \n",
    "        # Extraer y limpiar texto\n",
    "        contenido = \" \".join(\n",
    "            [pytesseract.image_to_string(img, lang='spa').strip().replace('\\n', ' ') \n",
    "             for img in images]\n",
    "        )\n",
    "        \n",
    "        # Tokenizar el texto en oraciones\n",
    "        oraciones = sent_tokenize(contenido, language='spanish')\n",
    "        \n",
    "        # Asignar ID único a cada oración\n",
    "        oraciones_ids = []\n",
    "        oraciones_texto = []\n",
    "        \n",
    "        for i, oracion in enumerate(oraciones):\n",
    "            oraciones_ids.append(f\"{id_asignado}_{i}\")  # ID único para cada oración\n",
    "            oraciones_texto.append(oracion)  # Texto de la oración\n",
    "        \n",
    "        # Crear DataFrame con solo los campos requeridos\n",
    "        data = {\n",
    "            'ID': [id_asignado] * len(oraciones),\n",
    "            'Oracion_ID': oraciones_ids,\n",
    "            'Oracion': oraciones_texto\n",
    "        }\n",
    "        df_oraciones = pd.DataFrame(data, columns=['ID', 'Oracion_ID', 'Oracion'])\n",
    "        \n",
    "        # Escribir el DataFrame al CSV (o concatenar al existente)\n",
    "        if os.path.exists(csv_file):\n",
    "            df_oraciones.to_csv(csv_file, mode='a', header=False, index=False, sep=';')\n",
    "        else:\n",
    "            df_oraciones.to_csv(csv_file, mode='w', header=True, index=False, sep=';')\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {ruta_pdf}: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Mapeo de archivos a IDs y nombres\n",
    "documentos = {\n",
    "    \"PARTIDO UNIDAD POPULAR _Plan de trabajo_.pdf\": {\"id\": 11, \"nombre\": \"PARTIDO UNIDAD POPULAR\"},\n",
    "    \"MOVIMIENTO DEMOCRACIA SÍ _Plan de trabajo_.pdf\": {\"id\": 13, \"nombre\": \"MOVIMIENTO DEMOCRACIA SÍ\"},\n",
    "    \"PARTIDO SOCIAL CRISTIANO _Plan de trabajo_.pdf\": {\"id\": 15, \"nombre\": \"PARTIDO SOCIAL CRISTIANO\"}   \n",
    "}\n",
    "\n",
    "# Procesar todos los documentos\n",
    "for archivo, datos in documentos.items():\n",
    "    ruta_completa = os.path.join(pdf_directory, archivo)\n",
    "    if os.path.exists(ruta_completa):\n",
    "        if procesar_pdf(ruta_completa, datos['id'], datos['nombre']):\n",
    "            print(f\"{archivo} procesado (ID {datos['id']})\")\n",
    "    else:\n",
    "        print(f\"Archivo no encontrado: {ruta_completa}\")\n",
    "\n",
    "print(\"\\nProceso completado.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# limpiar la columna oracion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo oraciones.csv procesado, limpiado y ordenado correctamente.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Función para limpiar el contenido del texto\n",
    "def clean_content(text):\n",
    "    text = text.lower()\n",
    "    # Eliminar viñetas comunes\n",
    "    text = re.sub(r\"[\\u2022\\u25CB\\u2023\\u2219\\u2022\\u25AA\\u25B6\\u25B7\\u25C6\\u2043\\u25B8\\u25BB\\u2660\\u25FE\\u25FB]\", \"\", text)\n",
    "    \n",
    "    # Eliminar (cid:...) - Referencias CID\n",
    "    text = re.sub(r'\\(cid:\\d+\\)', '', text)\n",
    "    \n",
    "    # Eliminar enumeraciones (números seguidos de punto)\n",
    "    text = re.sub(r'^\\d+\\.', '', text)  # Al inicio de la línea\n",
    "    text = re.sub(r'\\n\\d+\\.', '\\n', text)  # En medio del texto\n",
    "\n",
    "    # Eliminar la enumeración de página (ejemplo: 'Página 1', 'pág. 2', etc.)\n",
    "    text = re.sub(r'Página \\d+', '', text)\n",
    "    text = re.sub(r'pág\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'pag\\.\\s*\\d+', '', text)\n",
    "    text = re.sub(r'Page \\d+', '', text)\n",
    "    text = re.sub(r'page \\d+', '', text)\n",
    "\n",
    "    # Eliminar caracteres especiales no alfabéticos ni numéricos (como @, #, $, etc.)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "    # Reemplazar múltiples espacios con uno solo\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Eliminar espacios al inicio y final\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Leer el archivo CSV, limpiar el contenido de la columna \"Oracion\", ordenar por ID y guardar\n",
    "def limpiar_y_guardar_csv(csv_file):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    row['Oracion'] = clean_content(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  \n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Ordenar las filas por ID (conversión a int para evitar errores de ordenación)\n",
    "        filas_existentes.sort(key=lambda x: int(x['ID']))\n",
    "\n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(csv_file, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo {csv_file} procesado, limpiado y ordenado correctamente.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "limpiar_y_guardar_csv('oraciones.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CSV actualizado con éxito. Se agregaron las columnas 'Temas Clave'.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "# Descargar recursos necesarios de NLTK\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "# Cargar modelo de lenguaje en español\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Cargar el CSV con las oraciones\n",
    "df = pd.read_csv(\"oraciones.csv\", sep=\";\")\n",
    "\n",
    "# Lista ampliada de temas clave\n",
    "temas_relevantes = {\n",
    "    \"economía\", \"educación\", \"salud\", \"seguridad\", \"empleo\",\n",
    "    \"infraestructura\", \"corrupción\", \"tecnología\", \"ambiente\",\n",
    "    \"justicia\", \"transporte\", \"política\", \"desarrollo\", \"energía\",\n",
    "    \"derechos humanos\", \"igualdad\", \"innovación\", \"turismo\",\n",
    "    \"agricultura\", \"cultura\", \"deporte\", \"finanzas\", \"inversión\",\n",
    "    \"vivienda\", \"servicios públicos\", \"ciencia\", \"medio ambiente\",\n",
    "    \"gobierno\", \"industria\", \"exportaciones\", \"importaciones\",\n",
    "    \"educación superior\", \"sanidad\", \"movilidad\", \"inteligencia artificial\",\n",
    "    \"seguridad ciudadana\", \"crimen organizado\", \"democracia\", \"pobreza\",\n",
    "    \"sostenibilidad\", \"digitalización\", \"gestión pública\", \"comercio\",\n",
    "    \"cambio climático\", \"energías renovables\", \"transparencia\", \"ciberseguridad\",\n",
    "    \"salud pública\", \"gobernanza\", \"justicia social\", \"igualdad de género\",\n",
    "    \"emprendimiento\", \"industria 4.0\", \"desarrollo sostenible\", \"desastres naturales\",\n",
    "    \"reforestación\", \"movilidad urbana\", \"biodiversidad\", \"educación financiera\",\n",
    "    \"trabajo remoto\", \"accesibilidad\", \"industria alimentaria\", \"industria tecnológica\",\n",
    "    \"educación digital\", \"cultura digital\", \"sociedad del conocimiento\", \n",
    "    \"banca digital\", \"teletrabajo\", \"inteligencia colectiva\", \"biotecnología\",\n",
    "    \"blockchain\", \"fintech\", \"medicina personalizada\", \"economía circular\",\n",
    "    \"ciudades inteligentes\", \"protección de datos\", \"energía solar\", \"transporte eléctrico\",\n",
    "    \"robotización\", \"computación cuántica\", \"espacio exterior\", \"protección ambiental\",\n",
    "    \"seguridad en la nube\", \"movilidad eléctrica\", \"alimentos orgánicos\", \"tecnología educativa\",\n",
    "    \"agtech\", \"neurociencia\", \"edtech\", \"deep learning\", \"big data\", \"sistemas autónomos\",\n",
    "    \"tecnología espacial\", \"cambio de paradigma\", \"smart grids\", \"ciudades sostenibles\", \n",
    "    \"ecoeficiencia\", \"energía eólica\", \"tecnologías disruptivas\", \"energía geotérmica\",\n",
    "    \"nanotecnología\", \"microbioma\", \"bioeconomía\", \"ecoturismo\", \"industrias creativas\",\n",
    "    \"gobernanza digital\", \"energía limpia\", \"criptomonedas\", \"minería digital\", \"ciencias marinas\",\n",
    "    \"nanomateriales\", \"inteligencia emocional\", \"finanzas sostenibles\", \"educación en línea\",\n",
    "    \"biomimicry\", \"ecoinnovación\", \"simulación computacional\", \"agricultura urbana\", \"cultivos inteligentes\"\n",
    "}\n",
    "\n",
    "# Función para extraer solo los temas clave\n",
    "def extraer_temas_clave(texto):\n",
    "    if pd.isna(texto):  # Manejar valores nulos\n",
    "        return \"\"\n",
    "\n",
    "    oraciones = sent_tokenize(texto, language=\"spanish\")  # Dividir en oraciones\n",
    "    temas_detectados = set()\n",
    "\n",
    "    for oracion in oraciones:\n",
    "        # Identificar temas clave dentro de la oración\n",
    "        temas_detectados.update({tema for tema in temas_relevantes if tema in oracion.lower()})\n",
    "\n",
    "    # Retornar los temas clave detectados como una cadena separada por coma\n",
    "    return \", \".join(temas_detectados)\n",
    "\n",
    "# Aplicar la extracción de temas clave en cada fila del DataFrame\n",
    "df[\"Temas Clave\"] = df[\"Oracion\"].apply(extraer_temas_clave)\n",
    "\n",
    "# Guardar el nuevo CSV con la nueva columna 'Temas Clave' sin eliminar datos anteriores\n",
    "df.to_csv(\"oraciones_actualizado.csv\", index=False, sep=\";\")\n",
    "\n",
    "print(\" CSV actualizado con éxito. Se agregaron las columnas 'Temas Clave'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words, tokenizar,lemmatizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo procesado y guardado correctamente en: oraciones_procesadas.csv\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import csv\n",
    "import re\n",
    "import os\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar recursos necesarios\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Inicializar el lematizador de SpaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stopwords en español de NLTK\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "# Función para lematizar y eliminar stopwords\n",
    "def lematizar_y_eliminar_stopwords(texto):\n",
    "    doc = nlp(texto)\n",
    "    # Lematizar y eliminar stopwords\n",
    "    return ' '.join([token.lemma_ for token in doc if token.is_alpha and token.lemma_ not in stop_words])\n",
    "\n",
    "# Limpiar contenido eliminando puntuaciones y números\n",
    "def clean_content(texto):\n",
    "    texto = re.sub(r'[^\\w\\s]', '', texto)  # Eliminar puntuaciones\n",
    "    texto = re.sub(r'\\d+', '', texto)      # Eliminar números\n",
    "    return texto.lower()\n",
    "\n",
    "# Función para procesar el CSV\n",
    "def limpiar_y_guardar_csv(csv_file, output_csv):\n",
    "    try:\n",
    "        filas_existentes = []\n",
    "        \n",
    "        # Leer las filas existentes desde el CSV\n",
    "        if os.path.exists(csv_file):\n",
    "            with open(csv_file, 'r', encoding='utf-8-sig') as f:\n",
    "                reader = csv.DictReader(f, delimiter=';')\n",
    "                for row in reader:\n",
    "                    # Limpiar el contenido de la columna \"Oracion\"\n",
    "                    if 'Oracion' in row:\n",
    "                        row['Oracion'] = clean_content(row['Oracion'])\n",
    "                        # Procesar el contenido: lematizar y eliminar stopwords\n",
    "                        row['Oracion'] = lematizar_y_eliminar_stopwords(row['Oracion'])\n",
    "                    \n",
    "                    # Verificar si la columna \"Oracion\" no está vacía\n",
    "                    if row['Oracion']:  # Si no está vacío o solo contiene espacios\n",
    "                        filas_existentes.append(row)\n",
    "        \n",
    "        # Escribir las filas modificadas en el archivo CSV\n",
    "        with open(output_csv, 'w', newline='', encoding='utf-8-sig') as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=['ID', 'Oracion_ID', 'Oracion', 'Temas Clave'], delimiter=';')\n",
    "            writer.writeheader()\n",
    "            writer.writerows(filas_existentes)\n",
    "        \n",
    "        print(f\"Archivo procesado y guardado correctamente en: {output_csv}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando el archivo CSV: {str(e)}\")\n",
    "\n",
    "# Llamar a la función\n",
    "input_csv = 'oraciones_actualizado.csv'\n",
    "output_csv = 'oraciones_procesadas.csv'\n",
    "limpiar_y_guardar_csv(input_csv, output_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# enbeddings Bert y faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc66232ab6c94f179da7ad1e0d2f73a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/430 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings y FAISS guardados exitosamente.\n",
      "Resultados de la consulta:\n",
      "1. ID: 14 | Oración: mantenimiento modernización infraestructura mejorar eficiencia reducir pérdida sistema energético | Temas Clave: infraestructura, ciencia | Partido: PARTIDO AVANZA  (Distancia: 18.809154510498047)\n",
      "2. ID: 4 | Oración: eficiencia energético implementar política eficiencia energético sector clave industria transporte construcción promover uso tecnología reducir consumo energía | Temas Clave: energía, transporte, ciencia, tecnología, política, industria | Partido: MOVIMIENTO CENTRO DEMOCRÁTICO  (Distancia: 20.003599166870117)\n",
      "3. ID: 1 | Oración: intervenir vivienda promover eficiencia energético | Temas Clave: vivienda, ciencia | Partido: REVOLUCIÓN CIUDADANA - RETO  (Distancia: 20.277034759521484)\n",
      "4. ID: 8 | Oración: colaboración sector privado ser vital desarrollo sostenibilidad sistema energético | Temas Clave: desarrollo, sostenibilidad | Partido: MOVIMIENTO PUEBLO IGUALDAD DEMOCRACIA _PID_  (Distancia: 21.76897430419922)\n",
      "5. ID: 14 | Oración: campaña nacional eficiencia ahorro energético promover práctica eficiencia ahorro energético sector | Temas Clave: ciencia | Partido: PARTIDO AVANZA  (Distancia: 23.09795379638672)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV con las oraciones procesadas\n",
    "df = pd.read_csv('oraciones_procesadas.csv', delimiter=';')  # Cambia el nombre de tu archivo CSV si es necesario\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos y partidos\n",
    "candidatos_df = pd.read_csv('candidatos.csv', delimiter=';')  # Asegúrate de que contiene 'ID' y 'Nombre Partido'\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si está disponible\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Crear embeddings para todas las oraciones\n",
    "embeddings = model.encode(df['Oracion'].tolist(), show_progress_bar=True, device=device)\n",
    "\n",
    "# Convertir los embeddings en un formato compatible con FAISS (float32)\n",
    "embeddings = np.array(embeddings).astype('float32')\n",
    "\n",
    "# Crear un índice FAISS\n",
    "dimension = embeddings.shape[1]  # Dimensión de los embeddings\n",
    "index = faiss.IndexFlatL2(dimension)  # Índice basado en L2 (distancia euclidiana)\n",
    "index.add(embeddings)  # Agregar los embeddings al índice FAISS\n",
    "\n",
    "# Guardar los embeddings y el índice FAISS en archivos\n",
    "np.save('embeddings.npy', embeddings)  # Guardamos los embeddings en un archivo .npy\n",
    "faiss.write_index(index, 'faiss_index.index')  # Guardamos el índice FAISS en un archivo .index\n",
    "\n",
    "# Guardar los ID y las oraciones en un archivo .pkl para cargarlos fácilmente después\n",
    "with open('sentences.pkl', 'wb') as f:\n",
    "    pickle.dump(df[['ID', 'Oracion', 'Temas Clave']].to_dict(orient='records'), f)\n",
    "\n",
    "print(\"Embeddings y FAISS guardados exitosamente.\")\n",
    "\n",
    "# Cargar el modelo de spaCy para lematización en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para preprocesar el texto (limpieza y lematización)\n",
    "def preprocess_text(text):\n",
    "    # Tokenización\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    \n",
    "    # Eliminar stopwords y caracteres no alfabéticos\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    \n",
    "    # Lematización usando spaCy\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    \n",
    "    # Unir los tokens lematizados en una sola cadena\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Función para obtener el nombre del partido a partir del ID\n",
    "def get_partido_name(id):\n",
    "    partido_row = candidatos_df[candidatos_df['ID'] == id]\n",
    "    if not partido_row.empty:\n",
    "        return partido_row.iloc[0]['Partido']\n",
    "    return \"Partido no encontrado\"\n",
    "\n",
    "# Función para realizar una consulta\n",
    "def query_faiss(query, top_k=5):\n",
    "    # Preprocesar la consulta\n",
    "    cleaned_query = preprocess_text(query)\n",
    "\n",
    "    # Generar el embedding para la consulta\n",
    "    query_embedding = model.encode([cleaned_query], device=device)\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "\n",
    "    # Realizar la búsqueda en FAISS\n",
    "    D, I = index.search(query_embedding, top_k)  # D son las distancias, I son los índices\n",
    "\n",
    "    # Obtener los resultados\n",
    "    with open('sentences.pkl', 'rb') as f:\n",
    "        sentences = pickle.load(f)\n",
    "\n",
    "    # Mostrar los resultados\n",
    "    print(\"Resultados de la consulta:\")\n",
    "    for i in range(top_k):\n",
    "        result = sentences[I[0][i]]\n",
    "        partido_nombre = get_partido_name(result['ID'])\n",
    "        print(f\"{i+1}. ID: {result['ID']} | Oración: {result['Oracion']} | Temas Clave: {result['Temas Clave']} | Partido: {partido_nombre} (Distancia: {D[0][i]})\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "query = \"¿Cómo mejorar la eficiencia energética en la industria?\"\n",
    "query_faiss(query)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\kale\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basándonos en las declaraciones políticas proporcionadas, es posible identificar varios temas y partidos que se repiten y algunas características generales de cada uno.\n",
      "\n",
      "**Temas Clave**\n",
      "\n",
      "* Empleo: Es un tema central en la mayoría de las declaraciones, con énfasis en generar empleo, mejorar la calidad del empleo y fortalecer relaciones entre el gobierno y el sector privado.\n",
      "* Desarrollo sostenible: También es un tema importante, con énfasis en lograr desarrollo económico y social sostenibles a largo plazo.\n",
      "* Seguridad jurídica: Muchos partidos destacan la importancia de garantizar una seguridad jurídica para su gente.\n",
      "* Democracia y transparencia: La mayoría de los partidos mencionados enfatizan la importancia de promover la democracia, la justicia y la transparencia en el gobierno.\n",
      "\n",
      "**Partidos Políticos**\n",
      "\n",
      "* **PARTIDO SOCIAL CRISTIANO**: Destaca la importancia del desarrollo sostenible, el empleo y la seguridad jurídica. También enfatiza la necesidad de garantizar una mayor representación de los ciudadanos en el proceso electoral.\n",
      "* **MOVIMIENTO CONSTRUYE**: Se centra en el impulso de un cambio social y político para abordar problemas como la pobreza, la desigualdad y la corrupción. Destaca también la importancia del empleo y la generación de empleos.\n",
      "* **MOVIMIENTO CENTRO DEMOCRÁTICO**: Se enfoca en la defensa de los derechos civiles y sociales, así como en la promoción de una democracia participativa y transparente.\n",
      "* **PARTIDO AVANZA**: Destaca la importancia de un desarrollo económico sostenible, así como de una mayor igualdad de oportunidades para todos los ciudadanos.\n",
      "\n",
      "**Candidatos**\n",
      "\n",
      "La mayoría de las declaraciones se refieren a candidatos que buscan ganar elecciones o representar a partidos políticos específicos. Algunos de los nombres mencionados son:\n",
      "\n",
      "* Iván Patricio Sáquicela Roldán ( Movimiento Democracia Sí)\n",
      "* María Luisa Coello Recalde (Movimiento Democracia Sí)\n",
      "* Wilson Enrique Gómez Váscones (Partido Sociedad Unida Más Acción, Suma)\n",
      "* Inés Rocio Díaz Chirar (Partido Sociedad Unida Más Acción, Suma)\n",
      "\n",
      "**Desafíos y Oportunidades**\n",
      "\n",
      "Algunos de los desafíos y oportunidades mencionados en las declaraciones son:\n",
      "\n",
      "* La necesidad de promover la democracia y la transparencia en el gobierno.\n",
      "* La importancia de abordar problemas como la pobreza, la desigualdad y la corrupción.\n",
      "* La necesidad de garantizar una seguridad jurídica para los ciudadanos.\n",
      "* La oportunidad de impulsar un desarrollo económico sostenible y equitativo.\n",
      "\n",
      "En resumen, las declaraciones políticas proporcionadas destacan la importancia de la democracia, la transparencia, el empleo y el desarrollo sostenible. También enfatizan la necesidad de garantizar una seguridad jurídica para los ciudadanos y abordar problemas como la pobreza, la desigualdad y la corrupción. Algunos partidos políticos destacados incluyen el Partido Social Cristiano, el Movimiento Construye, el Movimiento Centro Democrático y el Partido Avanza.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import pickle\n",
    "import nltk\n",
    "import spacy\n",
    "import ollama  # Importar la librería para usar Ollama\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Descargar recursos de NLTK\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Verificar si hay GPU disponible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Cargar el archivo CSV con las oraciones procesadas\n",
    "df = pd.read_csv('oraciones_procesadas.csv', delimiter=';')\n",
    "\n",
    "# Cargar el archivo CSV con los candidatos y partidos\n",
    "candidatos_df = pd.read_csv('candidatos.csv', delimiter=';')\n",
    "\n",
    "# Inicializar el modelo BERT preentrenado y moverlo a la GPU si está disponible\n",
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# Cargar el índice FAISS previamente guardado\n",
    "index = faiss.read_index('faiss_index.index')\n",
    "\n",
    "# Cargar los datos de oraciones\n",
    "with open('sentences.pkl', 'rb') as f:\n",
    "    sentences = pickle.load(f)\n",
    "\n",
    "# Cargar el modelo de spaCy para lematización en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Función para preprocesar el texto\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower(), language='spanish')\n",
    "    stop_words = set(stopwords.words('spanish'))\n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
    "    doc = nlp(' '.join(tokens))\n",
    "    lemmatized_tokens = [token.lemma_ for token in doc]\n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Función para obtener el nombre del partido a partir del ID\n",
    "def get_partido_name(id):\n",
    "    partido_row = candidatos_df[candidatos_df['ID'] == id]\n",
    "    if not partido_row.empty:\n",
    "        return partido_row.iloc[0]['Partido']\n",
    "    return \"Partido no encontrado\"\n",
    "\n",
    "# Función para realizar una consulta en FAISS y obtener una respuesta con Ollama\n",
    "def query_faiss_ollama(query, top_k=20):\n",
    "    cleaned_query = preprocess_text(query)\n",
    "    query_embedding = model.encode([cleaned_query], device=device).astype('float32')\n",
    "    D, I = index.search(query_embedding, top_k)\n",
    "\n",
    "    resultados = []\n",
    "    for i in range(top_k):\n",
    "        result = sentences[I[0][i]]\n",
    "        partido_nombre = get_partido_name(result['ID'])\n",
    "        resultados.append(f\"Oración: {result['Oracion']}, Temas Clave: {result['Temas Clave']}, Partido: {partido_nombre}\")\n",
    "\n",
    "    # Construir el prompt para Ollama\n",
    "    prompt = f\"He encontrado las siguientes declaraciones políticas relacionadas:\\n\\n\" + \"\\n\".join(resultados) + \"\\n\\nGenera un resumen basado en estas declaraciones explicame a profundidad.\"\n",
    "\n",
    "    # Generar la respuesta con Ollama\n",
    "    respuesta = ollama.chat(model='llama3.2:latest', messages=[{'role': 'user', 'content': prompt}])\n",
    "\n",
    "    return respuesta['message']['content']\n",
    "\n",
    "\n",
    "query = \"que candidaato propone generacion de empleo a los recien graduados\"\n",
    "respuesta = query_faiss_ollama(query)\n",
    "print(respuesta)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
